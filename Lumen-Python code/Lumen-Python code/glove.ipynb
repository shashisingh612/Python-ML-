{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-423359fa1aa3>, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-423359fa1aa3>\"\u001b[1;36m, line \u001b[1;32m66\u001b[0m\n\u001b[1;33m    w = np.random.randn(V,D)/np.sqrt(V+D)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#'C:\\\\Users\\\\ac36345\\\\Desktop\\\\nlp code\\\\glove.6B\\\\glove.6B.50d.txt',encoding=\"utf8\"\n",
    "import numpy as np\n",
    "import json \n",
    "import os\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.util import shuffle\n",
    "from word2vec import get_wikipedia_data,find_analogies\n",
    "\n",
    "class Glove:\n",
    "    def __init__(self,D,V,contect_sz):\n",
    "        self.D = D \n",
    "        self.V = V\n",
    "        self.context_sz = context_sz\n",
    "    def fit(self,sentences,cc_matrix=None,learning_rate=10e-5,reg=0.1,xmax=100,alpha=0.75,epochs=10,gd=False,use_theano=True):\n",
    "        t0 = datetime.now()\n",
    "        V=self.V\n",
    "        D=self.Dn\n",
    "        if not os.path.exists(cc_matrix):\n",
    "            x = np.zeros((V,V))\n",
    "            N = len(sentences)\n",
    "            print(\"number of sentences of process:\",N)\n",
    "            it =0\n",
    "            for sentence in sentences:\n",
    "                it +=1\n",
    "                if it % 10000 ==0:\n",
    "                    print(\"processed\",it,\"/\",N)\n",
    "                n = len(sentence)\n",
    "                for i in xrange(n):\n",
    "                    wi = sentence[i]\n",
    "                # i,j is not the word ind\n",
    "                    start = max(0,i-self.context_sz)\n",
    "                    end =   min(n,i+self.context_sz)\n",
    "                    if i-self.context_sz < 0 :\n",
    "                        points = 1.0 / (i+1)\n",
    "                        X[wi,0] +=points\n",
    "                        X[0,wi] +=points\n",
    "                    if i+self.context_sz > n:\n",
    "                        points = 1.0 / (n-i)\n",
    "                        X[wi,1] +=points\n",
    "                        X[1,wi] +=points\n",
    "                    #left side\n",
    "                    for j in xrange(start , i):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 /(i-j)\n",
    "                        X[wi,wj] +=points\n",
    "                        X[wj,wi] +=points\n",
    "                   #right side\n",
    "                    for j in xrange(i+1,end):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 /(j-i)\n",
    "                        X[wi,wj] +=points\n",
    "                        X[wj,wi] +=points\n",
    "            np.save(cc_matrix,X)\n",
    "        else:\n",
    "            X = np.load(cc_matrix)\n",
    "            print(\"max value in X:\",X.max())\n",
    "            fx = np.zeros((V,V))\n",
    "            fx[X,xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
    "            fx[X >= xmax] = 1\n",
    "            logX = np.log(X+1)\n",
    "            print(\"time to build co-occurrence matrix:\",(datetime.now() - t0)\n",
    "            w = np.random.randn(V,D)/np.sqrt(V+D)\n",
    "            b = np.zeros(V)\n",
    "            U = np.random.randn(V,D)/np.sqrt(V+D)\n",
    "            c = np.zeros(V)\n",
    "            mu = logX.mean()\n",
    "            if gd and use_theano:\n",
    "                  pass\n",
    "          costs = []\n",
    "          sentence_indexes = range(len(sentences))\n",
    "          for epoch in xrange(epochs):\n",
    "              delta = W.dot(U.T) +b.reshape(V,1) + c.reshape(1,V) + mu - logX\n",
    "              cost = (fX * delta * delta).sum()\n",
    "              costs.append(cost)\n",
    "              print(\"epoch:\",epoch,\"cost:\",cost)\n",
    "              if gd:\n",
    "               # print('******')\n",
    "                if use_theano:\n",
    "                    pass\n",
    "                else: \n",
    "                    oldW =  W.copy()\n",
    "                    for i in xrange(V):\n",
    "                        W[i] -= learning_rate * (fX[i,:]*delta[i,:]).dot(U)\n",
    "                        W -= learning_rate*reg*W\n",
    "                    for i in xrange(V):\n",
    "                        b[i] -= learning_rate * fX[i,:].dot(delta[i,:])\n",
    "                        b -= learning_rate*reg*b\n",
    "                    for j in xrange(V):\n",
    "                        U[i] -= learning_rate * (fX[:,j]*delta[:,j]).dot(oldW)\n",
    "                        U -= learning_rate*reg*U\n",
    "                    for j in xrange(V):\n",
    "                        c[j] -= learning_rate *fX[:,j].dot(delta[:,j])\n",
    "                        c -= learning_rate*reg*c\n",
    "               # print('&&&&&')\n",
    "              else:\n",
    "                pass\n",
    "          self.w = w\n",
    "          self.U = U\n",
    "          plt.plot(costs)\n",
    "          plt.show()\n",
    "      def save(self,fn):\n",
    "          arrays = [self.W,self.U.T]\n",
    "          np.savex(fn,*arrays)\n",
    "\n",
    "    def main(we_file,w2i_file,n_files=50):\n",
    "        cc_matrix = \"cc_matrix_%s.npy\" %n_files\n",
    "        if os.path.exists(cc_matrix):\n",
    "            with open(w2i_file) as f:\n",
    "                word2idx =  json.load(f)\n",
    "            sentences =[]\n",
    "        else:\n",
    "            sentences,word2idx = get_wikipedia_data(n_files=n_files,n_vocab=2000)\n",
    "            with open(w2i_file,'w') as f:\n",
    "                json.dump(word2idx,f)\n",
    "        \n",
    "        V = len(word2idx)\n",
    "        model = Glove(80,V,10)\n",
    "        model.fit(\n",
    "             sentences=sentences,\n",
    "             cc_matrix = cc_matrix,\n",
    "             learning_rate = 3*10e-5,\n",
    "             reg = 0.01,\n",
    "             epochs = 2000,\n",
    "             gd = True,\n",
    "             use_theano = False,\n",
    "        )\n",
    "        model.save(we_file)\n",
    "    if __name__ == '__main__':\n",
    "        we ='glove_model_50.npz'\n",
    "        w2i = 'glove_model_50.json'\n",
    "        main(we,w2i)\n",
    "        for concat in (True,False):\n",
    "            print(\"** concat:\",concat)\n",
    "            find_analogies('king','man','woman',concat,we,w2i)\n",
    "\n",
    "                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "from builtins import range\n",
    "from nltk.corpus import brown\n",
    "import operator\n",
    "\n",
    "KEEP_WORDS = set([\n",
    "'king','man','queen','woman',\n",
    "'italy','rome','france','paris',\n",
    "'london','britain','england',])\n",
    "\n",
    "def get_sentences():\n",
    "    return brown.sents()\n",
    "\n",
    "def get_sentences_with_word2idx():\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "    i = 2\n",
    "    word2idx = {'START':0,'END': 1}\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = i\n",
    "                i +=1\n",
    "            indexed_sentence.append(word2idx[token])\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "    print('vocab size:',i)\n",
    "    return indexed_sentences,word2idx\n",
    "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000,keep_words=KEEP_WORDS):\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "    i = 2\n",
    "    word2idx = {'START':0,'END': 1}\n",
    "    idx2word = ['START','END']\n",
    "    word_idx_count = {\n",
    "      0: float('inf'),\n",
    "      1: float('inf'),\n",
    "    }\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                idx2word.append(token)\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "            idx = word2idx[token]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx,0) + 1\n",
    "            indexed_sentence.append(idx)\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "    for word in keep_words:\n",
    "        word_idx_count[word2idx[word]] = float('inf')\n",
    "        # sort technique operator.itemgetter(1)\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key = operator.itemgetter(1),reverse = True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "\n",
    "    for idx,count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        print(word, count)\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx +=1\n",
    "    word2idx_small['UNKNOWN'] = new_idx\n",
    "    unknown = new_idx\n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    for word in keep_words:\n",
    "        assert(word in word2idx_small)\n",
    "    sentences_small = []\n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "    return sentences_small, word2idx_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
