{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range, input\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "#from rnn_class.util import get_wikipedia_data\n",
    "#from rnn_class.brown import get_sentences_with_word2idx_limit_vocab,get_sentences_with_word2idx\n",
    "\n",
    "#from markov import get_bigram_probs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sentences,word2idx = get_sentences_with_word2idx_limit_vocab(2000)\n",
    "    V =  len(word2idx)\n",
    "    print(\"Vocab size:\", V)\n",
    "    start_idx = word2idx['START']\n",
    "    end_idx = word2idx['END']\n",
    "    bigram_probs = get_bigram_probs(sentences,V,start_idx,end_idx,smoothing=0.1)\n",
    "    D=100\n",
    "    W1 = np.random.randn(V,D)/np.sqrt(V)\n",
    "    W2 = np.random.randn(D,V)/np.sqrt(D)\n",
    "\n",
    "    losses = []\n",
    "    epochs = 1\n",
    "    lr = 1e-1\n",
    "\n",
    "def softmax(a):\n",
    "    a = a - a.max()\n",
    "    exp_a = np.exp(a)\n",
    "    return exp_a / exp_a.sum(axis=1, keepdims = True)\n",
    "W_bigram = np.log(bigram_probs)\n",
    "bigram_losses = []\n",
    "\n",
    "t0 = datetime.now()\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(sentences)\n",
    "    j = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = [start_idx] + sentence +[end_idx]\n",
    "        n = len(sentence)\n",
    "      #  inputs = np.zeros((n-1,V))\n",
    "      #  targets  = np.zeros((n-1,V))\n",
    "      #  inputs[np.arange(n-1),sentence[:n-1]] = 1\n",
    "      #  targets[np.arange(n-1),sentence[1:]] = 1\n",
    "        inputs = sentence[:n-1]\n",
    "        targets = sentence[1:]\n",
    "        #hidden = np.tanh(inputs.dot(W1))\n",
    "        hidden = np.tanh(W1[inputs])\n",
    "        predictions = softmax(hidden.dot(W2))\n",
    "        loss = -np.sum(np.log(predictions[np.arange(n-1),targets]))/(n-1)\n",
    "        losses.append(loss)\n",
    "        doutput = predictions\n",
    "        doutput[np.arange(n-1),targets] -= 1\n",
    "        W2 = W2 - lr * hidden.T.dot(doutput)\n",
    "        dhidden = doutput.dot(W2.T)*(1-hidden * hidden )\n",
    "     # predictions = softmax(hidden.dot(W2))\n",
    "     # W2 = W2 - lr * hidden.T.dot(predictions -  targets)\n",
    "     # dhidden = (predictions - targets).dot(W2.T) * (1- hidden * hidden)\n",
    "     # W1 =  W1 - lr * inputs.T.dot(dhidden)\n",
    "\n",
    "\n",
    "     #loss = -np.sum(targets* np.log(predictions)) / (n-1)\n",
    "     #losses.append(loss)\n",
    "###addedbelow lines##\n",
    "        i = 0\n",
    "        for w in inputs:\n",
    "            W1[w] = W1[w] - lr*dhidden[i]\n",
    "            i +=1\n",
    "            \n",
    "##till here\n",
    "\n",
    "if epoch == 0:\n",
    "    bigram_predictions = softmax((W_bigram[inputs])\n",
    "    bigram_loss = -np.sum(np.log(bigram_predictions[np.arange(n-1),targets]))/(n-1)\n",
    "    #bigram_predictions = softmax(inputs.dot(W_bigram))\n",
    "    #bigram_loss = -np.sum(targets * np.log(bigram_predictions))/(n-1)                             \n",
    "    bigram_losses.append(bigram_loss)\n",
    "\n",
    "if j % 100 ==0:\n",
    "    print(\"epoch:\",\"sentence: %s /%s\" %(j, len(sentences)), \"loss:\",loss)\n",
    "j +=1\n",
    "\n",
    "print(\"elaped time training:\", datetime.now() - t0)\n",
    "plt.plot(losses)\n",
    "\n",
    "avg_bigram_loss = np.mean(bigram_losses)\n",
    "print(\"avg_bigram_loss:\", avg_bigram_loss)\n",
    "plt.axhline(y=avg_bigram_loss,color = 'r',linestyle='-')\n",
    "\n",
    "def smoothed_loss(x,decay=0.99):\n",
    "    y = np.zeros(len(x))\n",
    "    last = 0\n",
    "    for t in range(len(x)):\n",
    "        z = decay * last + (1- decay)* x[t]\n",
    "        y[t] = z /(1-decay **(t +1))\n",
    "        last = z\n",
    "    return y\n",
    "plt.plot(smoothed_loss(losses))\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"logistic model\")\n",
    "plt.imshow(softmax(W2))\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Bigram probs\")\n",
    "plt.imshow(bigram_probs)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_bigram_probs(sentences,V, start_idx, end_idx, smoothing=1):\n",
    "    bigram_probs = np.ones((V,V)) * smoothing\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            if i ==0:\n",
    "                bigram_probs[start_idx, sentence[i]] +=1\n",
    "            else:\n",
    "                bigram_probs[sentence[i-1],sentence[i]] +=1\n",
    "\n",
    "            if i == len(sentence) - 1:\n",
    "                bigram_probs[sentence[i], end_idx] +=1\n",
    "    bigram_probs /= bigram_probs.sum(axis=1,keepdims = True)\n",
    "    return bigram_probs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sentences,word2idx = get_sentences_with_word2idx_limit_vocab(10000)\n",
    "V = len(word2idx)\n",
    "print(\"Vocab size:\" , V)\n",
    "\n",
    "start_idx = word2idx['START']\n",
    "end_idx = word2idx['END']\n",
    "bigram_probs = get_bigram_probs(sentences, V ,start_idx, end_idx, smoothing= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "from builtins import range\n",
    "from nltk.corpus import brown\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "KEEP_WORDS = set([\n",
    "'king','man','queen','woman',\n",
    "'italy','rome','france','paris',\n",
    "'london','britain','england',])\n",
    "\n",
    "def get_sentences():\n",
    "    return brown.sents()\n",
    "\n",
    "def get_sentences_with_word2idx():\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "    i = 2\n",
    "    word2idx = {'START':0,'END': 1}\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = i\n",
    "                i +=1\n",
    "            indexed_sentence.append(word2idx[token])\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "    print('vocab size:',i)\n",
    "    return indexed_sentences,word2idx\n",
    "\n",
    "                \n",
    "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000,keep_words=KEEP_WORDS):\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "    i = 2\n",
    "    word2idx = {'START':0,'END': 1}\n",
    "    idx2word = ['START','END']\n",
    "    word_idx_count = {\n",
    "      0: float('inf'),\n",
    "      1: float('inf'),\n",
    "    }\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                idx2word.append(token)\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "            idx = word2idx[token]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx,0) + 1\n",
    "            indexed_sentence.append(idx)\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "    for word in keep_words:\n",
    "        word_idx_count[word2idx[word]] = float('inf')\n",
    "        # sort technique operator.itemgetter(1)\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key = operator.itemgetter(1),reverse = True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "\n",
    "    for idx,count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        print(word, count)\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx +=1\n",
    "    word2idx_small['UNKNOWN'] = new_idx\n",
    "    unknown = new_idx\n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    for word in keep_words:\n",
    "        assert(word in word2idx_small)\n",
    "    sentences_small = []\n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "    return sentences_small, word2idx_small"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
