{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import math\n",
    "import bokeh \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from dateutil import parser\n",
    "#from bokeh.layouts import gridplot\n",
    "#from bokeh.plotting import figure, show, output_file\n",
    "#from bokeh.layouts import row, column\n",
    "#from bokeh.resources import INLINE\n",
    "#from bokeh.io import output_notebook\n",
    "#from bokeh.models import Span\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#output_notebook(resources=INLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_world_gdp_with_codes.csv')\n",
    "country_codes = country_codes.drop('GDP (BILLIONS)', 1)\n",
    "country_codes.rename(columns={'COUNTRY': 'Country', 'CODE': 'Code'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_codes.sort_index().sort_index(axis=1)\n",
    "#virus_data.sort_index().sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_data = pd.read_csv('novel-corona-virus-2019-dataset/covid_19_data.csv')\n",
    "\n",
    "prev_index = 0\n",
    "first_time = False\n",
    "tmp = 0\n",
    "\n",
    "\n",
    "for i, row in virus_data.iterrows():\n",
    "\n",
    "    if(virus_data.loc[i,'SNo'] < 1342 and virus_data.loc[i,'Province/State']=='Hubei'):\n",
    "        if(first_time):\n",
    "            tmp = virus_data.loc[i,'Confirmed']\n",
    "            prev_index = i\n",
    "            virus_data.loc[i,'Confirmed'] = virus_data.loc[i,'Confirmed'] + 593\n",
    "            first_time = False\n",
    "        else:\n",
    "            increment = virus_data.loc[i,'Confirmed'] - tmp\n",
    "            tmp = virus_data.loc[i,'Confirmed']\n",
    "            virus_data.loc[i,'Confirmed'] = virus_data.loc[prev_index,'Confirmed'] + increment + 593\n",
    "            prev_index = i\n",
    "    \n",
    "\n",
    "virus_data.rename(columns={'Country/Region': 'Country', 'ObservationDate': 'Date'}, inplace=True)\n",
    "virus_data = virus_data.fillna('unknow')\n",
    "virus_data['Country'] = virus_data['Country'].str.replace('US','United States')\n",
    "virus_data['Country'] = virus_data['Country'].str.replace('UK','United Kingdom') \n",
    "virus_data['Country'] = virus_data['Country'].str.replace('Mainland China','China')\n",
    "virus_data['Country'] = virus_data['Country'].str.replace('South Korea','Korea, South')\n",
    "virus_data['Country'] = virus_data['Country'].str.replace('North Korea','Korea, North')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_data = pd.merge(virus_data,country_codes, on='Country', how='left',sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "unexpected attribute 'legend_label' to Line, possible attributes are js_event_callbacks, js_property_callbacks, line_alpha, line_cap, line_color, line_dash, line_dash_offset, line_join, line_width, name, subscribed_events, tags, x or y",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-92f8589997c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m p1.line(np.log(list(china['Confirmed'])), np.log(new_confirmed_cases_china), color='#DBAE23', \n\u001b[1;32m--> 107\u001b[1;33m         legend_label='China', line_width=1)\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[0mp1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcircle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchina\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Confirmed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_confirmed_cases_china\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"white\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mfakesource\u001b[0m in \u001b[0;36mline\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bokeh\\plotting\\helpers.py\u001b[0m in \u001b[0;36mfunc\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    818\u001b[0m             \u001b[0mmglyph_ca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 820\u001b[1;33m         \u001b[0mglyph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_glyph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglyphclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglyph_ca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    821\u001b[0m         \u001b[0mnsglyph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_glyph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglyphclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnsglyph_ca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m         \u001b[0msglyph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_glyph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglyphclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msglyph_ca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bokeh\\plotting\\helpers.py\u001b[0m in \u001b[0;36m_make_glyph\u001b[1;34m(glyphclass, kws, extra)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[0mkws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[0mkws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextra\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mglyphclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bokeh\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m         \u001b[0mdefault_theme\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_to_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bokeh\\core\\has_props.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **properties)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m             \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bokeh\\core\\has_props.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             raise AttributeError(\"unexpected attribute '%s' to %s, %s attributes are %s\" %\n\u001b[1;32m--> 288\u001b[1;33m                 (name, self.__class__.__name__, text, nice_join(matches)))\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: unexpected attribute 'legend_label' to Line, possible attributes are js_event_callbacks, js_property_callbacks, line_alpha, line_cap, line_color, line_dash, line_dash_offset, line_join, line_width, name, subscribed_events, tags, x or y"
     ]
    }
   ],
   "source": [
    "top_country = virus_data.loc[virus_data['Date'] == virus_data['Date'].iloc[-1]]\n",
    "top_country = top_country.groupby(['Code','Country'])['Confirmed'].sum().reset_index() \n",
    "top_country = top_country.sort_values('Confirmed', ascending=False)\n",
    "top_country = top_country[:30]\n",
    "top_country_codes = top_country['Country']\n",
    "top_country_codes = list(top_country_codes)\n",
    "\n",
    "#countries = virus_data.loc[virus_data['Country'] in top_country_codes]\n",
    "countries = virus_data[virus_data['Country'].isin(top_country_codes)]\n",
    "countries_day = countries.groupby(['Date','Code','Country'])['Confirmed','Deaths','Recovered'].sum().reset_index()\n",
    "\n",
    "\n",
    "exponential_line_x = []\n",
    "exponential_line_y = []\n",
    "for i in range(16):\n",
    "    exponential_line_x.append(i)\n",
    "    exponential_line_y.append(i)\n",
    "    \n",
    "## TOP 30 ##\n",
    "china = countries_day.loc[countries_day['Code']=='CHN']\n",
    "\n",
    "new_confirmed_cases_china = []\n",
    "new_confirmed_cases_china.append( list(china['Confirmed'])[0] - list(china['Deaths'])[0] \n",
    "                           - list(china['Recovered'])[0] )\n",
    "\n",
    "for i in range(1,len(china)):\n",
    "\n",
    "    new_confirmed_cases_china.append( list(china['Confirmed'])[i] - \n",
    "                                     list(china['Deaths'])[i] - \n",
    "                                     list(china['Recovered'])[i])\n",
    "    \n",
    "    \n",
    "italy = countries_day.loc[countries_day['Code']=='ITA']\n",
    "\n",
    "new_confirmed_cases_ita = []\n",
    "new_confirmed_cases_ita.append( list(italy['Confirmed'])[0] - list(italy['Deaths'])[0] \n",
    "                           - list(italy['Recovered'])[0] )\n",
    "\n",
    "for i in range(1,len(italy)):\n",
    "    \n",
    "    new_confirmed_cases_ita.append( list(italy['Confirmed'])[i] - \n",
    "                                  list(italy['Deaths'])[i] - \n",
    "                                  list(italy['Recovered'])[i])\n",
    "    \n",
    "    \n",
    "india = countries_day.loc[countries_day['Code']=='IND']\n",
    "\n",
    "new_confirmed_cases_india = []\n",
    "new_confirmed_cases_india.append( list(india['Confirmed'])[0] - list(india['Deaths'])[0] \n",
    "                           - list(india['Recovered'])[0] )\n",
    "\n",
    "for i in range(1,len(india)):\n",
    "    \n",
    "    new_confirmed_cases_india.append( list(india['Confirmed'])[i] - \n",
    "                                     list(india['Deaths'])[i] - \n",
    "                                    list(india['Recovered'])[i])\n",
    "    \n",
    "\n",
    "spain = countries_day.loc[countries_day['Code']=='ESP']\n",
    "\n",
    "new_confirmed_cases_spain = []\n",
    "new_confirmed_cases_spain.append( list(spain['Confirmed'])[0] - list(spain['Deaths'])[0] \n",
    "                           - list(spain['Recovered'])[0] )\n",
    "\n",
    "for i in range(1,len(spain)):\n",
    "    \n",
    "    new_confirmed_cases_spain.append( list(spain['Confirmed'])[i] - \n",
    "                                     list(spain['Deaths'])[i] - \n",
    "                                    list(spain['Recovered'])[i])\n",
    "    \n",
    "\n",
    "us = countries_day.loc[countries_day['Code']=='USA']\n",
    "\n",
    "new_confirmed_cases_us = []\n",
    "new_confirmed_cases_us.append( list(us['Confirmed'])[0] - list(us['Deaths'])[0] \n",
    "                           - list(us['Recovered'])[0] )\n",
    "\n",
    "for i in range(1,len(us)):\n",
    "    \n",
    "    new_confirmed_cases_us.append( list(us['Confirmed'])[i] - \n",
    "                                     list(us['Deaths'])[i] - \n",
    "                                    list(us['Recovered'])[i])\n",
    "    \n",
    "    \n",
    "german = countries_day.loc[countries_day['Code']=='DEU']\n",
    "\n",
    "new_confirmed_cases_german = []\n",
    "new_confirmed_cases_german.append( list(german['Confirmed'])[0] - list(german['Deaths'])[0] \n",
    "                           - list(german['Recovered'])[0] )\n",
    "\n",
    "for i in range(1,len(german)):\n",
    "    \n",
    "    new_confirmed_cases_german.append( list(german['Confirmed'])[i] - \n",
    "                                     list(german['Deaths'])[i] - \n",
    "                                    list(german['Recovered'])[i])\n",
    "    \n",
    "p1 = figure(plot_width=800, plot_height=550, title=\"Trajectory of Covid-19\")\n",
    "p1.grid.grid_line_alpha=0.3\n",
    "p1.ygrid.band_fill_color = \"olive\"\n",
    "p1.ygrid.band_fill_alpha = 0.1\n",
    "p1.xaxis.axis_label = 'Total number of detected cases (Log scale)'\n",
    "p1.yaxis.axis_label = 'New confirmed cases (Log scale)'\n",
    "\n",
    "p1.line(exponential_line_x, exponential_line_y, line_dash=\"4 4\", line_width=0.5)\n",
    "\n",
    "p1.line(np.log(list(china['Confirmed'])), np.log(new_confirmed_cases_china), color='#DBAE23', \n",
    "        legend_label='China', line_width=1)\n",
    "p1.circle(np.log(list(china['Confirmed'])[-1]), np.log(new_confirmed_cases_china[-1]), fill_color=\"white\", size=5)\n",
    "\n",
    "p1.line(np.log(list(italy['Confirmed'])), np.log(new_confirmed_cases_ita), color='#3EC358', \n",
    "        legend_label='Italy', line_width=1)\n",
    "p1.circle(np.log(list(italy['Confirmed'])[-1]), np.log(new_confirmed_cases_ita[-1]), fill_color=\"white\", size=5)\n",
    "\n",
    "#p1.line(np.log(list(corea_s['Confirmed'])), np.log(new_confirmed_cases_corea), color='#C3893E', \n",
    "#       legend_label='South Korea', line_width=1)\n",
    "#p1.circle(np.log(list(corea_s['Confirmed'])[-1]), np.log(new_confirmed_cases_corea[-1]), fill_color=\"white\", size=5)\n",
    "\n",
    "\n",
    "p1.line(np.log(list(india['Confirmed'])), np.log(new_confirmed_cases_india), color='#3E4CC3', \n",
    "        legend_label='India', line_width=1)\n",
    "p1.circle(np.log(list(india['Confirmed'])[-1]), np.log(new_confirmed_cases_india[-1]), fill_color=\"white\", size=5)\n",
    "\n",
    "p1.line(np.log(list(spain['Confirmed'])), np.log(new_confirmed_cases_spain), color='#F54138', \n",
    "        legend_label='Spain', line_width=1)\n",
    "p1.circle(np.log(list(spain['Confirmed'])[-1]), np.log(new_confirmed_cases_spain[-1]), fill_color=\"white\", size=5)\n",
    "\n",
    "p1.line(np.log(list(us['Confirmed'])), np.log(new_confirmed_cases_us), color='#23BCDB', \n",
    "        legend_label='United States', line_width=1)\n",
    "p1.circle(np.log(list(us['Confirmed'])[-1]), np.log(new_confirmed_cases_us[-1]), fill_color=\"white\", size=5)\n",
    "\n",
    "p1.line(np.log(list(german['Confirmed'])), np.log(new_confirmed_cases_german), color='#010A0C', \n",
    "        legend_label='Germany', line_width=1)\n",
    "p1.circle(np.log(list(german['Confirmed'])[-1]), np.log(new_confirmed_cases_german[-1]), fill_color=\"white\", size=5)\n",
    "\n",
    "p1.legend.location = \"bottom_right\"\n",
    "\n",
    "output_file(\"coronavirus.html\", title=\"coronavirus.py\")\n",
    "\n",
    "show(p1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\ac36345\\AppData\\Local\\Continuum\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - bokeh\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    bokeh-2.0.2                |             py_0         6.5 MB  bokeh\n",
      "    packaging-20.3             |             py_0          35 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         6.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  bokeh              bokeh/noarch::bokeh-2.0.2-py_0\n",
      "  packaging          pkgs/main/noarch::packaging-20.3-py_0\n",
      "  typing_extensions  pkgs/main/win-64::typing_extensions-3.7.4.1-py37_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "bokeh-2.0.2          | 6.5 MB    |            |   0% \n",
      "bokeh-2.0.2          | 6.5 MB    |            |   0% \n",
      "bokeh-2.0.2          | 6.5 MB    |            |   0% \n",
      "bokeh-2.0.2          | 6.5 MB    |            |   1% \n",
      "bokeh-2.0.2          | 6.5 MB    | 1          |   1% \n",
      "bokeh-2.0.2          | 6.5 MB    | 1          |   2% \n",
      "bokeh-2.0.2          | 6.5 MB    | 2          |   2% \n",
      "bokeh-2.0.2          | 6.5 MB    | 2          |   2% \n",
      "bokeh-2.0.2          | 6.5 MB    | 3          |   3% \n",
      "bokeh-2.0.2          | 6.5 MB    | 3          |   3% \n",
      "bokeh-2.0.2          | 6.5 MB    | 3          |   4% \n",
      "bokeh-2.0.2          | 6.5 MB    | 4          |   5% \n",
      "bokeh-2.0.2          | 6.5 MB    | 4          |   5% \n",
      "bokeh-2.0.2          | 6.5 MB    | 5          |   5% \n",
      "bokeh-2.0.2          | 6.5 MB    | 5          |   6% \n",
      "bokeh-2.0.2          | 6.5 MB    | 5          |   6% \n",
      "bokeh-2.0.2          | 6.5 MB    | 6          |   6% \n",
      "bokeh-2.0.2          | 6.5 MB    | 6          |   7% \n",
      "bokeh-2.0.2          | 6.5 MB    | 6          |   7% \n",
      "bokeh-2.0.2          | 6.5 MB    | 7          |   7% \n",
      "bokeh-2.0.2          | 6.5 MB    | 7          |   7% \n",
      "bokeh-2.0.2          | 6.5 MB    | 7          |   7% \n",
      "bokeh-2.0.2          | 6.5 MB    | 7          |   8% \n",
      "bokeh-2.0.2          | 6.5 MB    | 8          |   9% \n",
      "bokeh-2.0.2          | 6.5 MB    | 8          |   9% \n",
      "bokeh-2.0.2          | 6.5 MB    | 9          |   9% \n",
      "bokeh-2.0.2          | 6.5 MB    | 9          |   9% \n",
      "bokeh-2.0.2          | 6.5 MB    | 9          |  10% \n",
      "bokeh-2.0.2          | 6.5 MB    | 9          |  10% \n",
      "bokeh-2.0.2          | 6.5 MB    | #          |  10% \n",
      "bokeh-2.0.2          | 6.5 MB    | #          |  10% \n",
      "bokeh-2.0.2          | 6.5 MB    | #          |  11% \n",
      "bokeh-2.0.2          | 6.5 MB    | #          |  11% \n",
      "bokeh-2.0.2          | 6.5 MB    | #1         |  11% \n",
      "bokeh-2.0.2          | 6.5 MB    | #1         |  11% \n",
      "bokeh-2.0.2          | 6.5 MB    | #1         |  12% \n",
      "bokeh-2.0.2          | 6.5 MB    | #1         |  12% \n",
      "bokeh-2.0.2          | 6.5 MB    | #2         |  12% \n",
      "bokeh-2.0.2          | 6.5 MB    | #2         |  12% \n",
      "bokeh-2.0.2          | 6.5 MB    | #2         |  13% \n",
      "bokeh-2.0.2          | 6.5 MB    | #3         |  13% \n",
      "bokeh-2.0.2          | 6.5 MB    | #3         |  13% \n",
      "bokeh-2.0.2          | 6.5 MB    | #3         |  14% \n",
      "bokeh-2.0.2          | 6.5 MB    | #3         |  14% \n",
      "bokeh-2.0.2          | 6.5 MB    | #4         |  14% \n",
      "bokeh-2.0.2          | 6.5 MB    | #4         |  14% \n",
      "bokeh-2.0.2          | 6.5 MB    | #4         |  15% \n",
      "bokeh-2.0.2          | 6.5 MB    | #4         |  15% \n",
      "bokeh-2.0.2          | 6.5 MB    | #4         |  15% \n",
      "bokeh-2.0.2          | 6.5 MB    | #5         |  15% \n",
      "bokeh-2.0.2          | 6.5 MB    | #5         |  15% \n",
      "bokeh-2.0.2          | 6.5 MB    | #5         |  16% \n",
      "bokeh-2.0.2          | 6.5 MB    | #5         |  16% \n",
      "bokeh-2.0.2          | 6.5 MB    | #6         |  16% \n",
      "bokeh-2.0.2          | 6.5 MB    | #6         |  16% \n",
      "bokeh-2.0.2          | 6.5 MB    | #6         |  17% \n",
      "bokeh-2.0.2          | 6.5 MB    | #7         |  17% \n",
      "bokeh-2.0.2          | 6.5 MB    | #7         |  17% \n",
      "bokeh-2.0.2          | 6.5 MB    | #7         |  18% \n",
      "bokeh-2.0.2          | 6.5 MB    | #7         |  18% \n",
      "bokeh-2.0.2          | 6.5 MB    | #8         |  18% \n",
      "bokeh-2.0.2          | 6.5 MB    | #8         |  18% \n",
      "bokeh-2.0.2          | 6.5 MB    | #8         |  19% \n",
      "bokeh-2.0.2          | 6.5 MB    | #8         |  19% \n",
      "bokeh-2.0.2          | 6.5 MB    | #9         |  19% \n",
      "bokeh-2.0.2          | 6.5 MB    | #9         |  19% \n",
      "bokeh-2.0.2          | 6.5 MB    | #9         |  20% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##         |  20% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##         |  20% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##         |  21% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##1        |  21% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##1        |  21% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##1        |  22% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##1        |  22% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##2        |  22% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##2        |  22% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##2        |  23% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##2        |  23% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##3        |  23% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##3        |  23% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##3        |  24% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##3        |  24% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##4        |  24% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##4        |  24% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##4        |  25% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##5        |  25% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##5        |  26% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##5        |  26% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##6        |  26% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##6        |  26% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##6        |  27% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##7        |  27% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##7        |  27% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##7        |  28% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##7        |  28% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##8        |  28% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##8        |  29% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##9        |  29% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##9        |  30% \n",
      "bokeh-2.0.2          | 6.5 MB    | ##9        |  30% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###        |  30% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###        |  30% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###        |  31% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###1       |  31% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###1       |  32% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###1       |  32% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###2       |  32% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###2       |  32% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###2       |  33% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###3       |  34% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###3       |  34% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###4       |  34% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###4       |  35% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###5       |  35% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###5       |  36% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###5       |  36% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###6       |  36% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###6       |  37% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###6       |  37% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###7       |  37% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###7       |  37% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###7       |  38% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###8       |  38% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###8       |  39% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###8       |  39% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###9       |  39% \n",
      "bokeh-2.0.2          | 6.5 MB    | ###9       |  40% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####       |  40% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####       |  41% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####1      |  41% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####1      |  42% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####2      |  42% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####3      |  43% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####3      |  44% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####4      |  45% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####4      |  45% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####6      |  46% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####6      |  47% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####7      |  48% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####8      |  48% \n",
      "bokeh-2.0.2          | 6.5 MB    | ####9      |  49% \n",
      "bokeh-2.0.2          | 6.5 MB    | #####      |  50% \n",
      "bokeh-2.0.2          | 6.5 MB    | #####1     |  51% \n",
      "bokeh-2.0.2          | 6.5 MB    | #####1     |  52% \n",
      "bokeh-2.0.2          | 6.5 MB    | #####2     |  53% \n",
      "bokeh-2.0.2          | 6.5 MB    | #####3     |  54% \n",
      "bokeh-2.0.2          | 6.5 MB    | #####4     |  55% \n",
      "bokeh-2.0.2          | 6.5 MB    | #####6     |  56% \n",
      "bokeh-2.0.2          | 6.5 MB    | #####7     |  57% \n",
      "bokeh-2.0.2          | 6.5 MB    | ######     |  60% \n",
      "bokeh-2.0.2          | 6.5 MB    | ######1    |  62% \n",
      "bokeh-2.0.2          | 6.5 MB    | ######2    |  63% \n",
      "bokeh-2.0.2          | 6.5 MB    | ######4    |  64% \n",
      "bokeh-2.0.2          | 6.5 MB    | ######5    |  66% \n",
      "bokeh-2.0.2          | 6.5 MB    | ######7    |  68% \n",
      "bokeh-2.0.2          | 6.5 MB    | ######9    |  70% \n",
      "bokeh-2.0.2          | 6.5 MB    | #######1   |  71% \n",
      "bokeh-2.0.2          | 6.5 MB    | #######2   |  72% \n",
      "bokeh-2.0.2          | 6.5 MB    | #######2   |  73% \n",
      "bokeh-2.0.2          | 6.5 MB    | #######3   |  74% \n",
      "bokeh-2.0.2          | 6.5 MB    | #######4   |  74% \n",
      "bokeh-2.0.2          | 6.5 MB    | #######4   |  75% \n",
      "bokeh-2.0.2          | 6.5 MB    | #######5   |  76% \n",
      "bokeh-2.0.2          | 6.5 MB    | #######7   |  77% \n",
      "bokeh-2.0.2          | 6.5 MB    | #######8   |  78% \n",
      "bokeh-2.0.2          | 6.5 MB    | #######8   |  79% \n",
      "bokeh-2.0.2          | 6.5 MB    | #######9   |  80% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########   |  80% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########   |  81% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########1  |  81% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########1  |  82% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########1  |  82% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########2  |  83% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########3  |  83% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########4  |  84% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########4  |  85% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########5  |  85% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########5  |  86% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########6  |  86% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########6  |  87% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########7  |  87% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########7  |  88% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########8  |  88% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########8  |  89% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########9  |  89% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########9  |  90% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########  |  90% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########  |  91% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########1 |  91% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########1 |  92% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########2 |  92% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########2 |  93% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########3 |  93% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########5 |  95% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########5 |  96% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########6 |  96% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########6 |  97% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########7 |  97% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########7 |  98% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########8 |  98% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########8 |  99% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########9 |  99% \n",
      "bokeh-2.0.2          | 6.5 MB    | #########9 | 100% \n",
      "bokeh-2.0.2          | 6.5 MB    | ########## | 100% \n",
      "\n",
      "packaging-20.3       | 35 KB     |            |   0% \n",
      "packaging-20.3       | 35 KB     | ####6      |  46% \n",
      "packaging-20.3       | 35 KB     | ########## | 100% \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(140): Could not remove or rename C:\\Users\\ac36345\\AppData\\Local\\Continuum\\anaconda3\\pkgs\\bokeh-2.0.2-py_0.tar.bz2.  Please remove this file manually (you may need to reboot to free file handles)\n",
      "WARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(140): Could not remove or rename C:\\Users\\ac36345\\AppData\\Local\\Continuum\\anaconda3\\pkgs\\bokeh-2.0.2-py_0\\site-packages\\bokeh\\server\\static\\lib\\lib.webworker.d.ts.  Please remove this file manually (you may need to reboot to free file handles)\n",
      "WARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(140): Could not remove or rename C:\\Users\\ac36345\\AppData\\Local\\Continuum\\anaconda3\\pkgs\\bokeh-2.0.2-py_0\\site-packages\\bokeh\\server\\static\\lib\\lib.webworker.d.ts.  Please remove this file manually (you may need to reboot to free file handles)\n",
      "WARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(140): Could not remove or rename C:\\Users\\ac36345\\AppData\\Local\\Continuum\\anaconda3\\pkgs\\bokeh-2.0.2-py_0\\site-packages\\bokeh\\server\\static\\lib\\lib.webworker.d.ts.  Please remove this file manually (you may need to reboot to free file handles)\n",
      "\n",
      "InvalidArchiveError('Error with archive C:\\\\Users\\\\ac36345\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\pkgs\\\\bokeh-2.0.2-py_0.tar.bz2.  You probably need to delete and re-download or re-create this file.  Message from libarchive was:\\n\\nCould not unlink')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install -c bokeh bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bokeh.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Country'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Country'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-406692cb8b6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcountries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvirus_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvirus_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Country'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_country_codes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcountries_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcountries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Code'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Country'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Confirmed'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Deaths'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Recovered'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mexponential_line_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2994\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2995\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2996\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Country'"
     ]
    }
   ],
   "source": [
    "countries = virus_data[virus_data['Country'].isin(top_country_codes)]\n",
    "countries_day = countries.groupby(['Date','Code','Country'])['Confirmed','Deaths','Recovered'].sum().reset_index()\n",
    "\n",
    "\n",
    "exponential_line_x = []\n",
    "exponential_line_y = []\n",
    "for i in range(16):\n",
    "    exponential_line_x.append(i)\n",
    "    exponential_line_y.append(i)\n",
    "       \n",
    "india = countries_day.loc[countries_day['Code']=='IND']\n",
    "\n",
    "new_confirmed_cases_india = []\n",
    "new_confirmed_cases_india.append( list(india['Confirmed'])[0] - list(india['Deaths'])[0] \n",
    "                           - list(india['Recovered'])[0] )\n",
    "\n",
    "for i in range(1,len(india)):\n",
    "    \n",
    "    new_confirmed_cases_india.append( list(india['Confirmed'])[i] - \n",
    "                                     list(india['Deaths'])[i] - \n",
    "                                    list(india['Recovered'])[i])\n",
    "    \n",
    " \n",
    "p1 = figure(plot_width=800, plot_height=550, title=\"Trajectory of Covid-19 in India\")\n",
    "p1.grid.grid_line_alpha=0.3\n",
    "p1.ygrid.band_fill_color = \"olive\"\n",
    "p1.ygrid.band_fill_alpha = 0.1\n",
    "p1.xaxis.axis_label = 'Total number of detected cases (Log scale)'\n",
    "p1.yaxis.axis_label = 'New confirmed cases (Log scale)'\n",
    "p = figure(plot_width=400, plot_height=400)\n",
    "p.outline_line_width = 7\n",
    "p.outline_line_alpha = 0.3\n",
    "p.outline_line_color = \"navy\"\n",
    "\n",
    "p1.line(exponential_line_x, exponential_line_y, line_dash=\"4 4\", line_width=0.5)\n",
    "\n",
    "p1.line(np.log(list(india['Confirmed'])), np.log(new_confirmed_cases_india), color='#3E4CC3', \n",
    "        legend_label='India', line_width=1)\n",
    "p1.circle(np.log(list(india['Confirmed'])[-1]), np.log(new_confirmed_cases_india[-1]), fill_color=\"white\", size=5)\n",
    "\n",
    "p1.legend.location = \"bottom_right\"\n",
    "\n",
    "output_file(\"coronavirus_india.html\", title=\"India.py\")\n",
    "\n",
    "show(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly as py\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_data=pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv')\n",
    "choro_map=px.choropleth(corona_data, \n",
    "                    locations=\"Country/Region\", \n",
    "                    locationmode = \"country names\",\n",
    "                    color=\"Confirmed\", \n",
    "                    hover_name=\"Country/Region\", \n",
    "                    animation_frame=\"ObservationDate\"\n",
    "                   )\n",
    "\n",
    "choro_map.update_layout(\n",
    "    title_text = 'Global Spread of Coronavirus',\n",
    "    title_x = 0.5,\n",
    "    geo=dict(\n",
    "        showframe = False,\n",
    "        showcoastlines = False,\n",
    "    ))\n",
    "    \n",
    "choro_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "zone=pd.read_csv('/kaggle/input/covid-19-india-zone-classification/lockdownindiawarningzones.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "    nunique = df.nunique()\n",
    "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
    "    nRow, nCol = df.shape\n",
    "    columnNames = list(df)\n",
    "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
    "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "    for i in range(min(nCol, nGraphShown)):\n",
    "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
    "        columnDf = df.iloc[:, i]\n",
    "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
    "            valueCounts = columnDf.value_counts()\n",
    "            valueCounts.plot.bar()\n",
    "        else:\n",
    "            columnDf.hist()\n",
    "        plt.ylabel('counts')\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.title(f'{columnNames[i]}')\n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()\n",
    "    \n",
    "def plotCorrelationMatrix(df, graphWidth):\n",
    "    filename = df.dataframeName\n",
    "    df = df.dropna('columns') # drop columns with NaN\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    if df.shape[1] < 2:\n",
    "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
    "        return\n",
    "    corr = df.corr()\n",
    "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
    "    corrMat = plt.matshow(corr, fignum = 1)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.colorbar(corrMat)\n",
    "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "def plotScatterMatrix(df, plotSize, textSize):\n",
    "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
    "    # Remove rows and columns that would lead to df being singular\n",
    "    df = df.dropna('columns')\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    columnNames = list(df)\n",
    "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
    "        columnNames = columnNames[:10]\n",
    "    df = df[columnNames]\n",
    "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
    "    corrs = df.corr().values\n",
    "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
    "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
    "    plt.suptitle('Scatter and Density Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPerColumnDistribution(zone, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_India_cases = pd.read_csv('../input/covid19-in-india/covid_19_india.csv')\n",
    "covid_India_cases.rename(columns={'State/UnionTerritory': 'State', 'Cured': 'Recovered', 'Confirmed': 'Confirmed'}, inplace=True)\n",
    "\n",
    "statewise_cases = pd.DataFrame(covid_India_cases.groupby(['State'])['Confirmed', 'Deaths', 'Recovered'].max().reset_index())\n",
    "statewise_cases[\"Country\"] = \"India\" # in order to have a single root node\n",
    "fig = px.treemap(statewise_cases, path=['Country','State'], values='Confirmed',\n",
    "                  color='Confirmed', hover_data=['State'],\n",
    "                  color_continuous_scale='Rainbow')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.HTML('<div class=\"flourish-embed flourish-bar-chart-race\" data-src=\"visualisation/1977187\" data-url=\"https://flo.uri.sh/visualisation/1977187/embed\"><script src=\"https://public.flourish.studio/resources/embed.js\"></script></div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_India_cases = pd.read_csv('../input/covid19-in-india/covid_19_india.csv')\n",
    "covid_India_cases.rename(columns={'State/UnionTerritory': 'State', 'Cured': 'Recovered', 'Confirmed': 'Confirmed'}, inplace=True)\n",
    "\n",
    "statewise_cases = pd.DataFrame(covid_India_cases.groupby(['State'])['Confirmed', 'Deaths', 'Recovered'].max().reset_index())\n",
    "last=statewise_cases\n",
    "pos=pd.read_csv('../input/utm-of-india/UTM ZONES of INDIA.csv')\n",
    "ind_grp=last.merge(pos , left_on='State', right_on='State / Union Territory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "map = folium.Map(location=[20.5937, 78.9629], zoom_start=4,tiles='cartodbpositron')\n",
    "\n",
    "for lat, lon,state,Confirmed,Recovered,Deaths in zip(ind_grp['Latitude'], ind_grp['Longitude'],ind_grp['State'],ind_grp['Confirmed'],ind_grp['Recovered'],ind_grp['Deaths']):\n",
    "    folium.CircleMarker([lat, lon],\n",
    "                        radius=5,\n",
    "                        color='YlOrRd',\n",
    "                      popup =(\n",
    "                    'State: ' + str(state) + '<br>'\n",
    "                    'Confirmed: ' + str(Confirmed) + '<br>'\n",
    "                      'Recovered: ' + str(Recovered) + '<br>'\n",
    "                      'Deaths: ' + str(Deaths) + '<br>'),\n",
    "\n",
    "                        fill_color='red',\n",
    "                        fill_opacity=0.7 ).add_to(map)\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "m = folium.Map(location = [20.5937, 78.9629], zoom_start = 4,tiles='cartodbpositron',columns = ['State/UnionTerritory','Confirmed'],)\n",
    "\n",
    "heat_data = [[row['Latitude'],row['Longitude']] for index, row in ind_grp.iterrows()]\n",
    "HeatMap(heat_data,radius=16.5, blur = 6.5).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_map=pd.read_csv('../input/covid19-in-india/covid_19_india.csv')\n",
    "#ind_map.head()\n",
    "pos=pd.read_csv('../input/utm-of-india/UTM ZONES of INDIA.csv')\n",
    "ind_map1=ind_map.merge(pos , left_on='State/UnionTerritory', right_on='State / Union Territory')\n",
    "#ind_map1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind_map = ind_grp\n",
    "ind_map1  = ind_map1.groupby(['Date', 'State/UnionTerritory','Latitude','Longitude'])['Confirmed'].sum()\n",
    "\n",
    "\n",
    "ind_map1 = ind_map1.reset_index()\n",
    "ind_map1.head()\n",
    "ind_map1['size'] = ind_map1['Confirmed']*90000000\n",
    "ind_map1\n",
    "fig = px.scatter_mapbox(ind_map1, lat=\"Latitude\", lon=\"Longitude\",\n",
    "                     color=\"Confirmed\", size='size',hover_data=['State/UnionTerritory'],\n",
    "                     color_continuous_scale='burgyl', animation_frame=\"Date\", \n",
    "                     title='Spread total cases over time in India')\n",
    "fig.update(layout_coloraxis_showscale=True)\n",
    "fig.update_layout(mapbox_style=\"carto-positron\", mapbox_zoom=3, mapbox_center = {\"lat\":20.5937,\"lon\":78.9629})\n",
    "fig.update_layout(margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import squarify\n",
    "import plotly_express as px\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_details = pd.read_csv('../input/covid19-in-india/AgeGroupDetails.csv')\n",
    "india_covid_19 = pd.read_csv('../input/covid19-in-india/covid_19_india.csv')\n",
    "hospital_beds = pd.read_csv('../input/covid19-in-india/HospitalBedsIndia.csv')\n",
    "individual_details = pd.read_csv('../input/covid19-in-india/IndividualDetails.csv')\n",
    "ICMR_details = pd.read_csv('../input/covid19-in-india/ICMRTestingDetails.csv')\n",
    "ICMR_labs = pd.read_csv('../input/covid19-in-india/ICMRTestingLabs.csv')\n",
    "state_testing = pd.read_csv('../input/covid19-in-india/StatewiseTestingDetails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n",
    "deaths_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n",
    "recovered_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv')\n",
    "latest_data = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/04-04-2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_covid_19['Date'] = pd.to_datetime(india_covid_19['Date'])\n",
    "state_testing['Date'] = pd.to_datetime(state_testing['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_df = india_covid_19.groupby([\"State/UnionTerritory\", \"Date\"])[\"Confirmed\", \"Deaths\", \"Cured\"].sum().reset_index()\n",
    "cumulative_df[\"Date\"] = pd.to_datetime(cumulative_df[\"Date\"] , format=\"%m/%d/%Y\").dt.date\n",
    "cumulative_df = cumulative_df.sort_values(by=\"Date\").reset_index(drop=True)\n",
    "start_date = datetime.date(2020, 3, 10)\n",
    "cumulative_df = cumulative_df[cumulative_df[\"Date\"]>=start_date]\n",
    "cumulative_df[\"Date\"] = cumulative_df[\"Date\"].astype(str)\n",
    "\n",
    "fig = px.scatter(cumulative_df, x=\"Confirmed\", y=\"Deaths\", animation_frame=\"Date\", animation_group=\"State/UnionTerritory\",\n",
    "           size=\"Confirmed\", color=\"State/UnionTerritory\", hover_name=\"State/UnionTerritory\",\n",
    "           log_x=False, size_max=55, range_x=[0,15000], range_y=[-20,800])\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=go.layout.Title(\n",
    "        text=\"Changes in number of confirmed & death cases over time in India states\",\n",
    "        x=0.5\n",
    "    ),\n",
    "    font=dict(size=14),\n",
    "    xaxis_title = \"Total number of confirmed cases\",\n",
    "    yaxis_title = \"Total number of death cases\"\n",
    ")\n",
    "\n",
    "fig.update_layout(layout)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.pie(age_details, values='TotalCases', names='AgeGroup',title='Confirmed cases of India')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Missing', 'Male', 'Female']\n",
    "sizes = []\n",
    "sizes.append(individual_details['gender'].isnull().sum())\n",
    "sizes.append(list(individual_details['gender'].value_counts())[0])\n",
    "sizes.append(list(individual_details['gender'].value_counts())[1])\n",
    "\n",
    "explode = (0, 0.1, 0)\n",
    "colors = ['#ffcc99','#66b3ff','#ff9999']\n",
    "\n",
    "plt.figure(figsize= (15,10))\n",
    "plt.title('Percentage of Gender',fontsize = 20)\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Male', 'Female']\n",
    "sizes = []\n",
    "sizes.append(list(individual_details['gender'].value_counts())[0])\n",
    "sizes.append(list(individual_details['gender'].value_counts())[1])\n",
    "\n",
    "explode = (0.1, 0)\n",
    "colors = ['#66b3ff','#ff9999']\n",
    "\n",
    "plt.figure(figsize= (10,8))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "\n",
    "plt.title('Percentage of Gender (Ignoring the Missing Values)',fontsize = 20)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(confirmed_df.columns[4:])\n",
    "dates = list(pd.to_datetime(dates))\n",
    "dates_india = dates[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = confirmed_df.groupby('Country/Region').sum().reset_index()\n",
    "df2 = deaths_df.groupby('Country/Region').sum().reset_index()\n",
    "df3 = recovered_df.groupby('Country/Region').sum().reset_index()\n",
    "\n",
    "k = df1[df1['Country/Region']=='India'].loc[:,'1/30/20':]\n",
    "india_confirmed = k.values.tolist()[0] \n",
    "\n",
    "k = df2[df2['Country/Region']=='India'].loc[:,'1/30/20':]\n",
    "india_deaths = k.values.tolist()[0] \n",
    "\n",
    "k = df3[df3['Country/Region']=='India'].loc[:,'1/30/20':]\n",
    "india_recovered = k.values.tolist()[0] \n",
    "\n",
    "plt.figure(figsize= (15,10))\n",
    "plt.xticks(rotation = 90 ,fontsize = 11)\n",
    "plt.yticks(fontsize = 10)\n",
    "plt.xlabel(\"Dates\",fontsize = 20)\n",
    "plt.ylabel('Total cases',fontsize = 20)\n",
    "plt.title(\"Total Confirmed, Active, Death in India\" , fontsize = 20)\n",
    "\n",
    "ax1 = plt.plot_date(y= india_confirmed,x= dates_india,label = 'Confirmed',linestyle ='-',color = 'b')\n",
    "ax2 = plt.plot_date(y= india_recovered,x= dates_india,label = 'Recovered',linestyle ='-',color = 'g')\n",
    "ax3 = plt.plot_date(y= india_deaths,x= dates_india,label = 'Death',linestyle ='-',color = 'r')\n",
    "plt.legend();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/covid19-in-india/covid_19_india.csv')\n",
    "data = df.copy()\n",
    "data['Date'] = data['Date'].apply(pd.to_datetime)\n",
    "data.drop(['Sno', 'Time'],axis=1,inplace=True)\n",
    "\n",
    "# collect present data\n",
    "from datetime import date\n",
    "data_apr = data[data['Date'] > pd.Timestamp(date(2020,4,12))]\n",
    "\n",
    "# prepaing data state wise\n",
    "state_cases = data_apr.groupby('State/UnionTerritory')['Confirmed','Deaths','Cured'].max().reset_index()\n",
    "state_cases['Active'] = state_cases['Confirmed'] - (state_cases['Deaths']- state_cases['Cured'])\n",
    "state_cases[\"Death Rate (per 100)\"] = np.round(100*state_cases[\"Deaths\"]/state_cases[\"Confirmed\"],2)\n",
    "state_cases[\"Cure Rate (per 100)\"] = np.round(100*state_cases[\"Cured\"]/state_cases[\"Confirmed\"],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_details = pd.pivot_table(df, values=['Confirmed','Deaths','Cured'], index='State/UnionTerritory', aggfunc='max')\n",
    "state_details['Recovery Rate'] = round(state_details['Cured'] / state_details['Confirmed'],2)\n",
    "state_details['Deaths']['Madhya Pradesh#']=119\n",
    "\n",
    "state_details['Deaths']=state_details['Deaths'].astype(np.float32)\n",
    "#state_details=state_details.reset_index()\n",
    "state_details['Active']=state_details['Confirmed']-state_details['Cured']-state_details['Deaths']\n",
    "\n",
    "state_details['Death Rate'] = round(state_details['Deaths'] /state_details['Confirmed'], 2)\n",
    "state_details = state_details.sort_values(by='Confirmed', ascending= False)\n",
    "#state_details.style.background_gradient(cmap='PuBuGn')\n",
    "state_details.style.bar(subset=['Confirmed'], color='#FDD017')\\\n",
    "                    .bar(subset=['Cured'], color='lime')\\\n",
    "                    .bar(subset=['Deaths'], color='red')\\\n",
    "                    .bar(subset=['Active'], color='#0000FF')\\\n",
    "                    .bar(subset=['Recovery Rate'], color='#B1FB17')\\\n",
    "                    .bar(subset=['Death Rate'], color='#C0C0C0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_testing = pd.read_csv('../input/covid19-in-india/StatewiseTestingDetails.csv')\n",
    "state_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = pd.read_csv(\"../input/covid19-in-india/ICMRTestingLabs.csv\")\n",
    "fig = px.treemap(labs, path=['state','city'],\n",
    "                  color='city', hover_data=['lab','address'],\n",
    "                  color_continuous_scale='reds')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing=state_testing.groupby('State').sum().reset_index()\n",
    "testing=testing.sort_values(['TotalSamples'], ascending=True)\n",
    "fig = px.bar(testing, \n",
    "             x=\"TotalSamples\",\n",
    "             y=\"State\", \n",
    "             orientation='h',\n",
    "             height=800,\n",
    "             title='Testing statewise insight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laboratory\n",
    "import plotly.express as px\n",
    "values = list(ICMR_labs['state'].value_counts())\n",
    "names = list(ICMR_labs['state'].value_counts().index)\n",
    "df = pd.DataFrame(list(zip(values, names)), \n",
    "               columns =['values', 'names'])\n",
    "\n",
    "fig = px.bar(df, \n",
    "             x=\"values\", \n",
    "             y=\"names\", \n",
    "             orientation='h',\n",
    "             height=1000,\n",
    "             title='ICMR Testing Centers in each State')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hospital Bed\n",
    "plt.figure(figsize=(20,60))\n",
    "plt.subplot(4,1,1)\n",
    "hospital_beds=hospital_beds.sort_values('NumUrbanHospitals_NHP18', ascending= False)\n",
    "sns.barplot(data=hospital_beds,y='State/UT',x='NumUrbanHospitals_NHP18',color=sns.color_palette('RdBu')[0])\n",
    "plt.title('Urban Hospitals per states')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('States')\n",
    "for i in range(hospital_beds.shape[0]):\n",
    "    count = hospital_beds.iloc[i]['NumUrbanHospitals_NHP18']\n",
    "    plt.text(count+10,i,count,ha='center',va='center')\n",
    "\n",
    "plt.subplot(4,1,2)\n",
    "hospital_beds=hospital_beds.sort_values('NumRuralHospitals_NHP18', ascending= False)\n",
    "sns.barplot(data=hospital_beds,y='State/UT',x='NumRuralHospitals_NHP18',color=sns.color_palette('RdBu')[1])\n",
    "plt.title('Rural Hospitals per states')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('States')\n",
    "for i in range(hospital_beds.shape[0]):\n",
    "    count = hospital_beds.iloc[i]['NumRuralHospitals_NHP18']\n",
    "    plt.text(count+100,i,count,ha='center',va='center')\n",
    "\n",
    "plt.subplot(4,1,3)\n",
    "hospitalBeds=hospital_beds.sort_values('NumUrbanBeds_NHP18', ascending= False)\n",
    "sns.barplot(data=hospitalBeds,y='State/UT',x='NumUrbanBeds_NHP18',color=sns.color_palette('RdBu')[5])\n",
    "plt.title('Rural Beds per states')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('States')\n",
    "for i in range(hospitalBeds.shape[0]):\n",
    "    count = hospitalBeds.iloc[i]['NumUrbanBeds_NHP18']\n",
    "    plt.text(count+1500,i,count,ha='center',va='center')\n",
    "\n",
    "plt.subplot(4,1,4)\n",
    "hospitalBeds=hospitalBeds.sort_values('NumRuralBeds_NHP18', ascending= False)\n",
    "sns.barplot(data=hospitalBeds,y='State/UT',x='NumRuralBeds_NHP18',color=sns.color_palette('RdBu')[3])\n",
    "plt.title('Rural Beds per states')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('States')\n",
    "for i in range(hospitalBeds.shape[0]):\n",
    "    count = hospitalBeds.iloc[i]['NumRuralBeds_NHP18']\n",
    "    plt.text(count+1500,i,count,ha='center',va='center')\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contaminated Zones\n",
    "zone=pd.read_csv('/kaggle/input/covid-19-india-zone-classification/lockdownindiawarningzones.csv')\n",
    "zone.style.set_properties(**{'background-color': 'black',\n",
    "                           'color': 'lawngreen',\n",
    "                           'border-color': 'white'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "    nunique = df.nunique()\n",
    "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
    "    nRow, nCol = df.shape\n",
    "    columnNames = list(df)\n",
    "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
    "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "    for i in range(min(nCol, nGraphShown)):\n",
    "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
    "        columnDf = df.iloc[:, i]\n",
    "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
    "            valueCounts = columnDf.value_counts()\n",
    "            valueCounts.plot.bar()\n",
    "        else:\n",
    "            columnDf.hist()\n",
    "        plt.ylabel('counts')\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.title(f'{columnNames[i]}')\n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()\n",
    "    \n",
    "def plotCorrelationMatrix(df, graphWidth):\n",
    "    filename = df.dataframeName\n",
    "    df = df.dropna('columns') # drop columns with NaN\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    if df.shape[1] < 2:\n",
    "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
    "        return\n",
    "    corr = df.corr()\n",
    "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
    "    corrMat = plt.matshow(corr, fignum = 1)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.colorbar(corrMat)\n",
    "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "def plotScatterMatrix(df, plotSize, textSize):\n",
    "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
    "    # Remove rows and columns that would lead to df being singular\n",
    "    df = df.dropna('columns')\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    columnNames = list(df)\n",
    "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
    "        columnNames = columnNames[:10]\n",
    "    df = df[columnNames]\n",
    "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
    "    corrs = df.corr().values\n",
    "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
    "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
    "    plt.suptitle('Scatter and Density Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPerColumnDistribution(zone, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(zone, path=[\"State\",'Zone','District'],\n",
    "                  color='Zone',\n",
    "                 color_discrete_map={'(?)':'black','Green Zone':'#00FF00', 'Red Zone':'#ff0000', 'Orange Zone':'#FFA500'},\n",
    "                 hover_data=['Zone'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forcasing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "# Any results you write to the current directory are saved as output.\n",
    "train=pd.read_csv('/kaggle/input/coronavirus-2019ncov/covid-19-all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = train[train['Country/Region']=='India'].groupby('Date')['Confirmed','Deaths'].sum()\n",
    "country_df['day_count'] = list(range(1,len(country_df)+1))\n",
    "ydata = country_df.Confirmed\n",
    "xdata = country_df.day_count\n",
    "country_df['rate'] = (country_df.Confirmed-country_df.Confirmed.shift(1))/country_df.Confirmed\n",
    "country_df['increase'] = (country_df.Confirmed-country_df.Confirmed.shift(1))\n",
    "\n",
    "plt.plot(xdata, ydata, 'o')\n",
    "plt.title(\"India\")\n",
    "plt.ylabel(\"Population infected\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import pylab\n",
    "\n",
    "\n",
    "def sigmoid(x,c,a,b):\n",
    "     y = c*1 / (1 + np.exp(-a*(x-b)))\n",
    "     return y\n",
    "#country_df.ConfirmedCases\n",
    "#country_df.day_count\n",
    "xdata = np.array([1, 2, 3,4, 5, 6, 7])\n",
    "ydata = np.array([0, 0, 13, 35, 75, 89, 91])\n",
    "\n",
    "#([low_a,low_b],[high_a,high_b])\n",
    "#low x --> low b\n",
    "#high y --> high c\n",
    "#a is the sigmoidal shape.\n",
    "popt, pcov = curve_fit(sigmoid, xdata, ydata, method='dogbox',bounds=([0.,0., 0.],[100,2, 10.]))\n",
    "print(popt)\n",
    "\n",
    "x = np.linspace(-1, 10, 50)\n",
    "y = sigmoid(x, *popt)\n",
    "\n",
    "pylab.plot(xdata, ydata, 'o', label='data')\n",
    "pylab.plot(x,y, label='fit')\n",
    "pylab.ylim(-0.05, 105)\n",
    "pylab.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Sigmoid function,\n",
    "\n",
    "Here is a snap of how I learnt to fit Sigmoid Function - y = c/(1+np.exp(-a*(x-b))) and 3 coefficients [c, a, b]:\n",
    "\n",
    "c - the maximum value (eventual maximum infected people, the sigmoid scales to this value eventually)\n",
    "a - the sigmoidal shape (how the infection progress. The smaller, the softer the sigmoidal shape is)\n",
    "b - the point where sigmoid start to flatten from steepening (the midpoint of sigmoid, when the rate of increase start to slow down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = train[train['Country/Region']=='India'].groupby('Date')['Confirmed','Deaths','Recovered'].sum().reset_index(False)\n",
    "in_df['Active']=in_df['Confirmed']-in_df['Deaths']-in_df['Recovered']\n",
    "in_df = in_df[in_df.Active>=100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import pylab\n",
    "from datetime import timedelta\n",
    "\n",
    "in_df['day_count'] = list(range(1,len(in_df)+1))\n",
    "in_df['increase'] = (in_df.Active-in_df.Active.shift(1))\n",
    "in_df['rate'] = (in_df.Active-in_df.Active.shift(1))/in_df.Active\n",
    "\n",
    "\n",
    "def sigmoid(x,c,a,b):\n",
    "     y = c*1 / (1 + np.exp(-a*(x-b)))\n",
    "     return y\n",
    "\n",
    "xdata = np.array(list(in_df.day_count)[::2])\n",
    "ydata = np.array(list(in_df.Active)[::2])\n",
    "\n",
    "population=1.332*10**9\n",
    "popt, pcov = curve_fit(sigmoid, xdata, ydata, method='dogbox',bounds=([0.,0., 0.],[population,6, 100.]))\n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_a = 35500\n",
    "est_b = 0.14\n",
    "est_c = 39\n",
    "x = np.linspace(-1, in_df.day_count.max()+50, 50)\n",
    "y = sigmoid(x,est_a,est_b,est_c)\n",
    "pylab.plot(xdata, ydata, 'o', label='data')\n",
    "pylab.plot(x,y, label='fit',alpha = 0.6)\n",
    "pylab.ylim(-0.05, est_a*1.05)\n",
    "pylab.xlim(-0.05, est_c*2.05)\n",
    "pylab.legend(loc='best')\n",
    "plt.xlabel('days from day 1')\n",
    "plt.ylabel('confirmed cases')\n",
    "plt.title('India')\n",
    "pylab.show()\n",
    "\n",
    "\n",
    "#print('model start date:',in_df[in_df.day_count==1].index[0])\n",
    "#print('model start infection:',int(in_df[in_df.day_count==1].Active[0]))\n",
    "print('model fitted max Active at:',int(est_a))\n",
    "print('model sigmoidal coefficient is:',round(est_b,3))\n",
    "print('model curve stop steepening, start flattening by day:',int(est_c))\n",
    "print('model curve flattens by day:',int(est_c)*2)\n",
    "display(in_df.head(3))\n",
    "display(in_df.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "From this, its seen that in case of India if the graph goes like that:\n",
    "max Active case: 26500 \n",
    "curve stop steepening, start flattening by day: 39 ,which is: 22/04/2020\n",
    "curve flattens by day: 78 which is: 31/05/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEIR Model\n",
    "The SEIR models the flows of people between four states: susceptible (S), exposed (E), infected (I), and resistant (R).\n",
    "\n",
    "Each of those variables represents the number of people in those groups. The parameters alpha and beta partially control how fast people move from being susceptible to exposed (beta), from exposed to infected (sigma), and from infected to resistant (gamma). This model has two additional parameters; one is the background mortality (mu) which is unaffected by disease-state, while the other is vaccination (nu). The vaccination moves people from the susceptible to resistant directly, without becoming exposed or infected.\n",
    "\n",
    "The SEIR differs from the SIR model in the addition of a latency period. Individuals who are exposed (E) have had contact with an infected person, but are not themselves infectious.\n",
    "\n",
    "Instructions: The boxes on the right side of the page control the parameters of the model. The page should load with some parameters already in the box. Click \"submit\" to run the model. The parameters can all be modified and the model re-run. The parameters are Beta The parameter controlling how often a susceptible-infected contact results in a new exposure. Gamma The rate an infected recovers and moves into the resistant phase. Sigma The rate at which an exposed person becomes infective. Mu The natural mortality rate (this is unrelated to disease). This models a population of a constant size, Initial susceptible The number of susceptible individuals at the beginning of the model run. Initial exposed The number of exposed individuals at the beginning of the model run. Initial infected The number of infected individuals at the beginning of the model run. Initial recovered The number of recovered individuals at the beginning of the model run. Days Controls how long the model will run. This program runs on your computer, so some computers may run faster than others. It is probably a good idea not to set the number iterations or the initial populations too high, since it will take longer to run. Note that cookies must be enabled for the algorithm to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Details: This is an ordinary differential equation model, described by the following equation: derivative of S with respect to t equals The simulation uses the fourth-order Runge-Kutta algorithm to solve it numerically, with a step size fixed at 0.01, written in JavaScript. The plotting methods are from the flot module. Both the ode simulation and the script in this page calling it are new, so there may still be some unanticipated bugs (I am also fairly new to the language, so my code may be inefficient or bizarre in places). Internet Explorer may not work since it has not yet adopted the canvas element, which is used in plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.integrate import solve_ivp\n",
    "import numpy\n",
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Susceptible equation\n",
    "def dS_dt(S, I, R_t, T_inf):\n",
    "    return -(R_t / T_inf) * I * S\n",
    "\n",
    "# Exposed equation\n",
    "def dE_dt(S, E, I, R_t, T_inf, T_inc):\n",
    "    return (R_t / T_inf) * I * S - (T_inc**-1) * E\n",
    "\n",
    "# Infected equation\n",
    "def dI_dt(I, E, T_inc, T_inf):\n",
    "    return (T_inc**-1) * E - (T_inf**-1) * I\n",
    "\n",
    "# Recovered/Remove/deceased equation\n",
    "def dR_dt(I, T_inf):\n",
    "    return (T_inf**-1) * I\n",
    "\n",
    "def SEIR_model(t, y, R_t, T_inf, T_inc):\n",
    "    \n",
    "    if callable(R_t):\n",
    "        reproduction = R_t(t)\n",
    "    else:\n",
    "        reproduction = R_t\n",
    "        \n",
    "    S, E, I, R = y\n",
    "    \n",
    "    S_out = dS_dt(S, I, reproduction, T_inf)\n",
    "    E_out = dE_dt(S, E, I, reproduction, T_inf, T_inc)\n",
    "    I_out = dI_dt(I, E, T_inc, T_inf)\n",
    "    R_out = dR_dt(I, T_inf)\n",
    "    \n",
    "    return [S_out, E_out, I_out, R_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/covid19-global-forecasting-week-4/train.csv')\n",
    "test = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')\n",
    "train['Date_datetime'] = train['Date'].apply(lambda x: (datetime.datetime.strptime(x, '%Y-%m-%d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_info = pd.read_csv('/kaggle/input/covid19-population-data/population_data.csv')\n",
    "country_pop = pop_info.query('Type == \"Country/Region\"')\n",
    "province_pop = pop_info.query('Type == \"Province/State\"')\n",
    "country_lookup = dict(zip(country_pop['Name'], country_pop['Population']))\n",
    "province_lookup = dict(zip(province_pop['Name'], province_pop['Population']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_and_predict(data, pop, solution, title='SEIR model'):\n",
    "    sus, exp, inf, rec = solution.y\n",
    "    \n",
    "    f = plt.figure(figsize=(16,5))\n",
    "    ax = f.add_subplot(1,2,1)\n",
    "    #ax.plot(sus, 'b', label='Susceptible');\n",
    "    ax.plot(exp, 'y', label='Exposed');\n",
    "    ax.plot(inf, 'r', label='Infected');\n",
    "    ax.plot(rec, 'c', label='Recovered/deceased');\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Days\", fontsize=10);\n",
    "    plt.ylabel(\"Fraction of population\", fontsize=10);\n",
    "    plt.legend(loc='best');\n",
    "    \n",
    "    ax2 = f.add_subplot(1,2,2)\n",
    "    preds = np.clip((inf + rec) * pop ,0,np.inf)\n",
    "    ax2.plot(range(len(data)),preds[:len(data)],label = 'Predict ConfirmedCases')\n",
    "    ax2.plot(range(len(data)),data['ConfirmedCases'])\n",
    "    plt.title('Model predict and data')\n",
    "    plt.ylabel(\"Population\", fontsize=10);\n",
    "    plt.xlabel(\"Days\", fontsize=10);\n",
    "    plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Country = 'India'\n",
    "N = pop_info[pop_info['Name']==Country]['Population'].tolist()[0] # India Population \n",
    "\n",
    "# Load dataset of Hubei\n",
    "train_loc = train[train['Country_Region']==Country].query('ConfirmedCases > 0')\n",
    "if len(train_loc)==0:\n",
    "    train_loc = train[train['Province_State']==Country].query('ConfirmedCases > 0')\n",
    "\n",
    "n_infected = train_loc['ConfirmedCases'].iloc[0] # start from first comfirmedcase on dataset first date\n",
    "max_days = len(train_loc)# how many days want to predict\n",
    "\n",
    "# Initial stat for SEIR model\n",
    "s = (N - n_infected)/ N\n",
    "e = 0.\n",
    "i = n_infected / N\n",
    "r = 0.\n",
    "\n",
    "# Define all variable of SEIR model \n",
    "T_inc = 5.2  # average incubation period\n",
    "T_inf = 2.9 # average infectious period\n",
    "R_0 = 3.954 # reproduction number\n",
    "\n",
    "## Solve the SEIR model \n",
    "sol = solve_ivp(SEIR_model, [0, max_days], [s, e, i, r], args=(R_0, T_inf, T_inc), \n",
    "                t_eval=np.arange(max_days))\n",
    "\n",
    "## Plot result\n",
    "plot_model_and_predict(train_loc, N, sol, title = 'SEIR Model (without intervention)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all variable of SEIR model \n",
    "T_inc = 5.2  # average incubation period\n",
    "T_inf = 2.9  # average infectious period\n",
    "\n",
    "# Define the intervention parameters (fit result, latter will show how to fit)\n",
    "R_0, cfr, k, L=[ 3.95469597 , 0.04593316 , 3.      ,   15.32328881]\n",
    "\n",
    "def time_varying_reproduction(t): \n",
    "    return R_0 / (1 + (t/L)**k)\n",
    "\n",
    "sol2 = solve_ivp(SEIR_model, [0, max_days], [s, e, i, r], args=(time_varying_reproduction, T_inf, T_inc), \n",
    "                t_eval=np.arange(max_days))\n",
    "\n",
    "plot_model_and_predict(train_loc, N, sol2, title = 'SEIR Model (with intervention)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum_signal(vec):\n",
    "    temp_val = 0\n",
    "    vec_new = []\n",
    "    for i in vec:\n",
    "        if i > temp_val:\n",
    "            vec_new.append(i)\n",
    "            temp_val = i\n",
    "        else:\n",
    "            vec_new.append(temp_val)\n",
    "    return vec_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a constant reproduction number\n",
    "def eval_model_const(params, data, population, return_solution=False, forecast_days=0):\n",
    "    R_0, cfr = params # Paramaters, R0 and cfr \n",
    "    N = population # Population of each country\n",
    "    n_infected = data['ConfirmedCases'].iloc[0] # start from first comfirmedcase on dataset first date\n",
    "    max_days = len(data) + forecast_days # How many days want to predict\n",
    "    s, e, i, r = (N - n_infected)/ N, 0, n_infected / N, 0 #Initial stat for SEIR model\n",
    "    \n",
    "    # R0 become half after intervention days\n",
    "    def time_varying_reproduction(t):\n",
    "        if t > 80: # we set intervention days = 80\n",
    "            return R_0 * 0.5\n",
    "        else:\n",
    "            return R_0\n",
    "    \n",
    "    # Solve the SEIR differential equation.\n",
    "    sol = solve_ivp(SEIR_model, [0, max_days], [s, e, i, r], args=(time_varying_reproduction, T_inf, T_inc),\n",
    "                    t_eval=np.arange(0, max_days))\n",
    "    \n",
    "    sus, exp, inf, rec = sol.y\n",
    "    # Predict confirmedcase\n",
    "    y_pred_cases = np.clip((inf + rec) * N ,0,np.inf)\n",
    "    y_true_cases = data['ConfirmedCases'].values\n",
    "    \n",
    "    # Predict Fatalities by remove * fatality rate(cfr)\n",
    "    y_pred_fat = np.clip(rec*N* cfr, 0, np.inf)\n",
    "    y_true_fat = data['Fatalities'].values\n",
    "    \n",
    "    optim_days = min(20, len(data))  # Days to optimise for\n",
    "    weights = 1 / np.arange(1, optim_days+1)[::-1]  # Recent data is more heavily weighted\n",
    "    \n",
    "    # using mean squre log error to evaluate\n",
    "    msle_cases = mean_squared_log_error(y_true_cases[-optim_days:], y_pred_cases[-optim_days:], weights)\n",
    "    msle_fat = mean_squared_log_error(y_true_fat[-optim_days:], y_pred_fat[-optim_days:], weights)\n",
    "    msle_final = np.mean([msle_cases, msle_fat])\n",
    "    \n",
    "    if return_solution:\n",
    "        return msle_final, sol\n",
    "    else:\n",
    "        return msle_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Hill decayed reproduction number\n",
    "def eval_model_decay(params, data, population, return_solution=False, forecast_days=0):\n",
    "    R_0, cfr, k, L = params # Paramaters, R0 and cfr \n",
    "    N = population # Population of each country\n",
    "    n_infected = data['ConfirmedCases'].iloc[0] # start from first comfirmedcase on dataset first date\n",
    "    max_days = len(data) + forecast_days # How many days want to predict\n",
    "    s, e, i, r = (N - n_infected)/ N, 0, n_infected / N, 0 #Initial stat for SEIR model\n",
    "    \n",
    "    # https://github.com/SwissTPH/openmalaria/wiki/ModelDecayFunctions   \n",
    "    # Hill decay. Initial values: R_0=2.2, k=2, L=50\n",
    "    def time_varying_reproduction(t): \n",
    "        return R_0 / (1 + (t/L)**k)\n",
    "    \n",
    "    # Solve the SEIR differential equation.\n",
    "    sol = solve_ivp(SEIR_model, [0, max_days], [s, e, i, r], args=(time_varying_reproduction, T_inf, T_inc),\n",
    "                    t_eval=np.arange(0, max_days))\n",
    "    \n",
    "    sus, exp, inf, rec = sol.y\n",
    "    # Predict confirmedcase\n",
    "    y_pred_cases = np.clip((inf + rec) * N ,0,np.inf)\n",
    "    y_true_cases = data['ConfirmedCases'].values\n",
    "    \n",
    "    # Predict Fatalities by remove * fatality rate(cfr)\n",
    "    y_pred_fat = np.clip(rec*N* cfr, 0, np.inf)\n",
    "    y_true_fat = data['Fatalities'].values\n",
    "    \n",
    "    optim_days = min(20, len(data))  # Days to optimise for\n",
    "    weights = 1 / np.arange(1, optim_days+1)[::-1]  # Recent data is more heavily weighted\n",
    "    \n",
    "    # using mean squre log error to evaluate\n",
    "    msle_cases = mean_squared_log_error(y_true_cases[-optim_days:], y_pred_cases[-optim_days:], weights)\n",
    "    msle_fat = mean_squared_log_error(y_true_fat[-optim_days:], y_pred_fat[-optim_days:], weights)\n",
    "    msle_final = np.mean([msle_cases, msle_fat])\n",
    "    \n",
    "    if return_solution:\n",
    "        return msle_final, sol\n",
    "    else:\n",
    "        return msle_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train[-7:]),len(train[:-7]),len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from matplotlib import dates\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def fit_model_new(data, area_name, initial_guess=[2.2, 0.02, 2, 50], \n",
    "              bounds=((1, 20), (0, 0.15), (1, 3), (1, 100)), make_plot=True, decay_mode = None):\n",
    "    \n",
    "    if area_name in ['France']:# France last data looks weird, remove it\n",
    "        train = data.query('ConfirmedCases > 0').copy()[:-1]\n",
    "    #elif area_name in ['Virgin Islands']:\n",
    "    #    train = data[:-3].query('ConfirmedCases > 0').copy()\n",
    "    else:\n",
    "        train = data.query('ConfirmedCases > 0').copy()\n",
    "    \n",
    "    ####### Split Train & Valid #######\n",
    "    #valid_data = train[-1:]\n",
    "    train_data = train\n",
    "    \n",
    "    ####### If this country have no ConfirmedCase, return 0 #######\n",
    "    if len(train_data) == 0:\n",
    "        result_zero = np.zeros((43))\n",
    "        return pd.DataFrame({'ConfirmedCases':result_zero,'Fatalities':result_zero}), 0 \n",
    "    \n",
    "    ####### Load the population of area #######\n",
    "    try:\n",
    "        #population = province_lookup[area_name]\n",
    "        population = pop_info[pop_info['Name']==area_name]['Population'].tolist()[0]\n",
    "    except IndexError:\n",
    "        print ('country not in population set, '+str(area_name))\n",
    "        population = 1000000 \n",
    "    \n",
    "    \n",
    "    if area_name == 'US':\n",
    "        population = 327200000\n",
    "    if area_name == 'Global':\n",
    "        population = 7744240900\n",
    "        \n",
    "    cases_per_million = train_data['ConfirmedCases'].max() * 10**6 / population\n",
    "    n_infected = train_data['ConfirmedCases'].iloc[0]\n",
    "    \n",
    "    ####### Total case/popuplation below 1, reduce country population #######\n",
    "    if cases_per_million < 1:\n",
    "        #print ('reduce pop divide by 100')\n",
    "        population = population/100\n",
    "        \n",
    "    ####### Fit the real data by minimize the MSLE #######\n",
    "    res_const = minimize(eval_model_const, [2.2, 0.02], bounds=((1, 20), (0, 0.15)),\n",
    "                         args=(train_data, population, False),\n",
    "                         method='L-BFGS-B')\n",
    "\n",
    "    res_decay = minimize(eval_model_decay, initial_guess, bounds=bounds,\n",
    "                         args=(train_data, population, False),\n",
    "                         method='L-BFGS-B')\n",
    "    \n",
    "    ####### Align the date information #######\n",
    "    test_end = datetime.datetime.strptime('2020-05-07','%Y-%m-%d')\n",
    "    test_start = datetime.datetime.strptime('2020-03-26','%Y-%m-%d')\n",
    "    train_test = data[data.Date_datetime>=test_start]\n",
    "    test_period = (test_end - test_start).days\n",
    "    train_max = train_data.Date_datetime.max()\n",
    "    train_min = train_data.Date_datetime.min()\n",
    "    add_date = 0\n",
    "    delta_days =(test_end - train_max).days\n",
    "    train_add_time=[]\n",
    "\n",
    "    if train_min > test_start:\n",
    "        add_date = (train_min-test_start).days\n",
    "        last = train_min-timedelta(add_date)\n",
    "        train_add_time = np.arange(last, train_min, dtype='datetime64[D]').tolist()\n",
    "        train_add_time = pd.to_datetime(train_add_time)\n",
    "        dates_all = train_add_time.append(pd.to_datetime(np.arange(train_min, test_end+timedelta(1), dtype='datetime64[D]')))\n",
    "    else:\n",
    "        dates_all = pd.to_datetime(np.arange(train_min, test_end+timedelta(1), dtype='datetime64[D]'))\n",
    "\n",
    "\n",
    "    ####### Auto find the best decay function ####### \n",
    "    if decay_mode is None:\n",
    "        if res_const.fun < res_decay.fun :\n",
    "            msle, sol = eval_model_const(res_const.x, train_data, population, True, delta_days+add_date)\n",
    "            res = res_const\n",
    "\n",
    "        else:\n",
    "            msle, sol = eval_model_decay(res_decay.x, train_data, population, True, delta_days+add_date)\n",
    "            res = res_decay\n",
    "            R_0, cfr, k, L = res.x\n",
    "    else:\n",
    "        if decay_mode =='day_decay':\n",
    "            msle, sol = eval_model_const(res_const.x, train_data, population, True, delta_days+add_date)\n",
    "            res = res_const\n",
    "        else:\n",
    "            msle, sol = eval_model_decay(res_decay.x, train_data, population, True, delta_days+add_date)\n",
    "            res = res_decay\n",
    "            R_0, cfr, k, L = res.x\n",
    "\n",
    "    ####### Predict the result by using best fit paramater of SEIR model ####### \n",
    "    sus, exp, inf, rec = sol.y\n",
    "    \n",
    "    y_pred = pd.DataFrame({\n",
    "        'ConfirmedCases': cumsum_signal(np.diff((inf + rec) * population, prepend=n_infected).cumsum()),\n",
    "       # 'ConfirmedCases': [inf[0]*population for i in range(add_date)]+(np.clip((inf + rec) * population,0,np.inf)).tolist(),\n",
    "       # 'Fatalities': [rec[0]*population for i in range(add_date)]+(np.clip(rec, 0, np.inf) * population * res.x[1]).tolist()\n",
    "        'Fatalities': cumsum_signal((np.clip(rec * population * res.x[1], 0, np.inf)).tolist())\n",
    "    })\n",
    "\n",
    "    #y_pred_valid = y_pred.iloc[len(train_data):len(train_data)+len(valid_data)]\n",
    "    y_pred_valid = y_pred.iloc[:len(train_data)]\n",
    "    y_pred_test = pd.concat([train_test[['ConfirmedCases', 'Fatalities']],y_pred.iloc[-(delta_days):]], ignore_index=True)\n",
    "    y_true_valid = train_data[['ConfirmedCases', 'Fatalities']]\n",
    "    #y_true_valid = valid_data[['ConfirmedCases', 'Fatalities']]\n",
    "    #print (len(y_pred),train_min)\n",
    "    \n",
    "    ####### Calculate MSLE ####### \n",
    "    valid_msle_cases = mean_squared_log_error(y_true_valid['ConfirmedCases'], y_pred_valid['ConfirmedCases'])\n",
    "    valid_msle_fat = mean_squared_log_error(y_true_valid['Fatalities'], y_pred_valid['Fatalities'])\n",
    "    valid_msle = np.mean([valid_msle_cases, valid_msle_fat])\n",
    "    \n",
    "    ####### Plot the fit result of train data and forecast after 300 days ####### \n",
    "    if make_plot:\n",
    "        if len(res.x)<=2:\n",
    "            print(f'Validation MSLE: {valid_msle:0.5f}, using intervention days decay, Reproduction number(R0) : {res.x[0]:0.5f}, Fatal rate : {res.x[1]:0.5f}')\n",
    "        else:\n",
    "            print(f'Validation MSLE: {valid_msle:0.5f}, using Hill decay, Reproduction number(R0) : {res.x[0]:0.5f}, Fatal rate : {res.x[1]:0.5f}, K : {res.x[2]:0.5f}, L: {res.x[3]:0.5f}')\n",
    "        \n",
    "        ####### Plot the fit result of train data dna SEIR model trends #######\n",
    "\n",
    "        f = plt.figure(figsize=(16,5))\n",
    "        ax = f.add_subplot(1,2,1)\n",
    "        ax.plot(exp, 'y', label='Exposed');\n",
    "        ax.plot(inf, 'r', label='Infected');\n",
    "        ax.plot(rec, 'c', label='Recovered/deceased');\n",
    "        plt.title('SEIR Model Trends')\n",
    "        plt.xlabel(\"Days\", fontsize=10);\n",
    "        plt.ylabel(\"Fraction of population\", fontsize=10);\n",
    "        plt.legend(loc='best');\n",
    "        #train_date_remove_year = train_data['Date_datetime'].apply(lambda date:'{:%m-%d}'.format(date))\n",
    "        ax2 = f.add_subplot(1,2,2)\n",
    "        xaxis = train_data['Date_datetime'].tolist()\n",
    "        xaxis = dates.date2num(xaxis)\n",
    "        hfmt = dates.DateFormatter('%m\\n%d')\n",
    "        ax2.xaxis.set_major_formatter(hfmt)\n",
    "        ax2.plot(np.array(train_data['Date_datetime'], dtype='datetime64[D]'),train_data['ConfirmedCases'],label='Confirmed Cases (train)', c='g')\n",
    "        ax2.plot(np.array(train_data['Date_datetime'], dtype='datetime64[D]'), y_pred['ConfirmedCases'][:len(train_data)],label='Cumulative modeled infections', c='r')\n",
    "        #ax2.plot(np.array(valid_data['Date_datetime'], dtype='datetime64[D]'), y_true_valid['ConfirmedCases'],label='Confirmed Cases (valid)', c='b')\n",
    "        #ax2.plot(np.array(valid_data['Date_datetime'], dtype='datetime64[D]'),y_pred_valid['ConfirmedCases'],label='Cumulative modeled infections (valid)', c='y')\n",
    "        plt.title('Real ConfirmedCase and Predict ConfirmedCase')\n",
    "        plt.legend(loc='best');\n",
    "        plt.show()\n",
    "            \n",
    "        ####### Forecast 300 days after by using the best paramater of train data #######\n",
    "        if len(res.x)>2:\n",
    "            msle, sol = eval_model_decay(res.x, train_data, population, True, 300)\n",
    "        else:\n",
    "            msle, sol = eval_model_const(res.x, train_data, population, True, 300)\n",
    "        \n",
    "        sus, exp, inf, rec = sol.y\n",
    "        \n",
    "        y_pred = pd.DataFrame({\n",
    "            'ConfirmedCases': cumsum_signal(np.diff((inf + rec) * population, prepend=n_infected).cumsum()),\n",
    "            'Fatalities': cumsum_signal(np.clip(rec, 0, np.inf) * population * res.x[1])\n",
    "        })\n",
    "        \n",
    "        ####### Plot 300 days after of each country #######\n",
    "        start = train_min\n",
    "        end = start + timedelta(len(y_pred))\n",
    "        time_array = np.arange(start, end, dtype='datetime64[D]')\n",
    "\n",
    "        max_day = numpy.where(inf == numpy.amax(inf))[0][0]\n",
    "        where_time = time_array[max_day]\n",
    "        pred_max_day = y_pred['ConfirmedCases'][max_day]\n",
    "        xy_show_max_estimation = (where_time, max_day)\n",
    "        \n",
    "        con = y_pred['ConfirmedCases']\n",
    "        fat = y_pred['Fatalities']\n",
    "        max_day_con = numpy.where(con == numpy.amax(con))[0][0] # Find the max confimed case of each country\n",
    "        max_day_fat = numpy.where(fat == numpy.amax(fat))[0][0]\n",
    "        max_con = numpy.amax(con)\n",
    "        max_fat = numpy.amax(fat)\n",
    "        where_time_con = time_array[len(time_array)-50]\n",
    "        xy_show_max_estimation_confirmed = (where_time_con, max_con)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=time_array, y=y_pred['ConfirmedCases'].astype(int),\n",
    "                            mode='lines',\n",
    "                            line = dict(color='red'),\n",
    "                            name='Estimation Confirmed Case Start from '+ str(start.date())+ ' to ' +str(end.date())))\n",
    "        fig.add_trace(go.Scatter(x=time_array, y=y_pred['Fatalities'].astype(int),\n",
    "                            mode='lines',\n",
    "                            line = dict(color='yellow'),\n",
    "                            name='Estimation Fatalities Start from '+ str(start.date())+ ' to ' +str(end.date())))\n",
    "        fig.add_trace(go.Scatter(x=time_array[:len(train)], y=train['ConfirmedCases'],\n",
    "                            mode='lines',\n",
    "                            name='Confirmed case until '+ str(train_max.date()),line = dict(color='green', width=4)))\n",
    "        fig.add_trace(go.Scatter(x=time_array[:len(train)], y=train['Fatalities'],\n",
    "                            mode='lines',\n",
    "                            name='Fatalities case until '+ str(train_max.date()),line = dict(color='blue', width=4)))\n",
    "        fig.add_annotation(\n",
    "            x=where_time_con,\n",
    "            y=max_con-(max_con/30),\n",
    "            showarrow=False,\n",
    "            text=\"Estimate Max Case around:\" +str(int(max_con)),\n",
    "            font=dict(\n",
    "                color=\"Blue\",\n",
    "                size=15\n",
    "            ))\n",
    "        fig.add_annotation(\n",
    "            x=where_time_con,\n",
    "            y=max_fat-(max_fat/30),\n",
    "            showarrow=False,\n",
    "            text=\"Estimate Max death around:\" +str(int(max_fat)),\n",
    "            font=dict(\n",
    "                color=\"Blue\",\n",
    "                size=15\n",
    "            ))\n",
    "        fig.add_annotation(\n",
    "            x=time_array[len(train)-1],\n",
    "            y=train['ConfirmedCases'].tolist()[-1],\n",
    "            showarrow=True,\n",
    "            text=f\"Real Max ConfirmedCase: \" +str(int(train['ConfirmedCases'].tolist()[-1]))) \n",
    "        \n",
    "        fig.add_annotation(\n",
    "            x=time_array[len(train)-1],\n",
    "            y=train['Fatalities'].tolist()[-1],\n",
    "            showarrow=True,\n",
    "            text=f\"Real Max Fatalities: \" +str(int(train['Fatalities'].tolist()[-1]))) \n",
    "        \n",
    "        fig.add_annotation(\n",
    "            x=where_time,\n",
    "            y=pred_max_day,\n",
    "            text='Infect start decrease from: ' + str(where_time))   \n",
    "        fig.update_layout(title='Estimate Confirmed Case ,'+area_name+' Total population ='+ str(int(population)), legend_orientation=\"h\")\n",
    "        fig.show()\n",
    "        ###\n",
    "        df = pd.DataFrame({'Values': train_data['ConfirmedCases'].tolist()+y_pred['ConfirmedCases'].tolist(),'Date_datatime':time_array[:len(train_data)].tolist()+time_array.tolist(),\n",
    "                   'Real/Predict': ['ConfirmedCase' for i in range(len(train_data))]+['PredictCase' for i in range(len(y_pred))]})\n",
    "        fig = px.line(df, x=\"Date_datatime\", y=\"Values\",color = 'Real/Predict')\n",
    "        fig.show()\n",
    "        plt.figure(figsize = (16,7))\n",
    "        plt.plot(time_array[:len(train_data)],train_data['ConfirmedCases'],label='Confirmed case until '+ str(train_max.date()),color='g', linewidth=3.0)\n",
    "        plt.plot(time_array,y_pred['ConfirmedCases'],label='Estimation Confirmed Case Start from '+ str(start.date())+ ' to ' +str(end.date()),color='r', linewidth=1.0)\n",
    "        plt.annotate('Infect start decrease from: ' + str(where_time), xy=xy_show_max_estimation, size=15, color=\"black\")\n",
    "        plt.annotate('max Confirmedcase: ' + str(int(max_con)), xy=xy_show_max_estimation_confirmed, size=15, color=\"black\")\n",
    "        plt.title('Estimate Confirmed Case '+area_name+' Total population ='+ str(int(population)))\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    return y_pred_test, valid_msle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'India'\n",
    "if country not in train['Country_Region'].unique():\n",
    "    country_pd_train = train[train['Province_State']==country]\n",
    "else:\n",
    "    country_pd_train = train[train['Country_Region']==country]\n",
    "\n",
    "a,b = fit_model_new(country_pd_train,country,make_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Model\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.apply(lambda col: col.isnull().value_counts(), axis=0)\n",
    "test_df.apply(lambda col: col.isna().value_counts(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Province_State\"] = train_df[\"Province_State\"].fillna(\"\")\n",
    "test_df[\"Province_State\"] = test_df[\"Province_State\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Date\"] = pd.to_datetime(train_df[\"Date\"])\n",
    "test_df[\"Date\"] = pd.to_datetime(test_df[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"NewCases\"] = train_df.groupby([\"Country_Region\", \"Province_State\"])[\"ConfirmedCases\"].diff(periods=1)\n",
    "train_df[\"NewCases\"] = train_df[\"NewCases\"].fillna(0)\n",
    "train_df[\"NewCases\"] = np.where(train_df[\"NewCases\"] < 0, 0, train_df[\"NewCases\"])\n",
    "train_df[\"NewFatalities\"] = train_df.groupby([\"Country_Region\", \"Province_State\"])[\"Fatalities\"].diff(periods=1)\n",
    "train_df[\"NewFatalities\"] = train_df[\"NewFatalities\"].fillna(0)\n",
    "train_df[\"NewFatalities\"] = np.where(train_df[\"NewFatalities\"] < 0, 0, train_df[\"NewFatalities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"NewCases\"] = np.log(train_df[\"NewCases\"] + 1)\n",
    "train_df[\"NewFatalities\"] = np.log(train_df[\"NewFatalities\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(n_prev, n_next):\n",
    "    df = train_df.copy()\n",
    "    input_feats, output_feats = [], []\n",
    "    for i in range(1, n_prev+1):\n",
    "        for feat in [\"NewCases\", \"NewFatalities\"]:\n",
    "            df[\"{}_prev_{}\".format(feat, i)] = df.groupby([\"Country_Region\", \"Province_State\"])[feat].shift(i)\n",
    "            input_feats.append(\"{}_prev_{}\".format(feat, i))\n",
    "    \n",
    "    output_feats.extend([\"NewCases\", \"NewFatalities\"])\n",
    "    for i in range(1, n_next):\n",
    "        for feat in [\"NewCases\", \"NewFatalities\"]:\n",
    "            df[\"{}_next_{}\".format(feat, i)] = df.groupby([\"Country_Region\", \"Province_State\"])[feat].shift(-i)\n",
    "            output_feats.append(\"{}_next_{}\".format(feat, i))\n",
    "    df.dropna(inplace=True)       \n",
    "            \n",
    "    const_df = pd.get_dummies(df[[\"Province_State\", \"Country_Region\"]], drop_first=True)\n",
    "    time_df = df[input_feats]\n",
    "    time_df = time_df.values.reshape((df.shape[0],-1,2))\n",
    "    output_df = df[output_feats]\n",
    "    return const_df, time_df, output_df\n",
    "\n",
    "def preprocess_test(n_prev):\n",
    "    input_feats = []\n",
    "    append_df = pd.concat([train_df, test_df[test_df[\"Date\"] == train_df[\"Date\"].max() + timedelta(days=1)]])\n",
    "    append_df.sort_values([\"Country_Region\", \"Province_State\", \"Date\"], ascending=[True, True, True], inplace=True)\n",
    "    for i in range(1, n_prev+1):\n",
    "        for feat in [\"NewCases\", \"NewFatalities\"]:\n",
    "            append_df[\"{}_prev_{}\".format(feat, i)] = append_df.groupby([\"Country_Region\", \"Province_State\"])[feat].shift(i)\n",
    "            input_feats.append(\"{}_prev_{}\".format(feat, i))\n",
    "    append_df = append_df[append_df[\"ForecastId\"].notnull()]\n",
    "            \n",
    "    const_df = pd.get_dummies(append_df[[\"Province_State\", \"Country_Region\"]], drop_first=True)\n",
    "    time_df = append_df[input_feats]\n",
    "    time_df = time_df.values.reshape((append_df.shape[0],-1,2))\n",
    "    return const_df, time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_next = (test_df[\"Date\"].max() - train_df[\"Date\"].max()).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "const_df, time_df, output_df = preprocess_train(n_next, n_next)\n",
    "const_test_df, time_test_df = preprocess_test(n_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "time_input = Input(shape=(time_df.shape[1], time_df.shape[2]))\n",
    "lstm = layers.LSTM(128)(time_input)\n",
    "\n",
    "const_input = Input(shape=(const_df.shape[1],))\n",
    "\n",
    "combine = layers.concatenate([lstm, const_input], axis=-1)\n",
    "#lstm_out = layers.Dropout(0.1)(combine)\n",
    "output = layers.Dense(output_df.shape[1], activation='softmax')(combine)\n",
    "\n",
    "model = Model([time_input, const_input], output)\n",
    "#optimizer=optimizers.SGD(lr=0.01, nesterov=True)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([time_df, const_df], output_df, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict([time_test_df, const_test_df])\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test_df = test_df[test_df[\"Date\"] > train_df[\"Date\"].max()]\n",
    "sub_test_df = pd.concat([sub_test_df,\n",
    "                         pd.DataFrame(output.reshape((-1, 2)), columns=[\"NewCases\", \"NewFatalities\"], index=sub_test_df.index)],\n",
    "                         axis=1)\n",
    "sub_test_df[\"NewCases\"] = np.exp(sub_test_df[\"NewCases\"]) - 1\n",
    "sub_test_df[\"NewFatalities\"] = np.exp(sub_test_df[\"NewFatalities\"]) - 1\n",
    "sub_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_test_df = test_df[test_df[\"Date\"] <= train_df[\"Date\"].max()].merge(train_df[train_df[\"Date\"] >= test_df[\"Date\"].min()][[\"Province_State\",\"Country_Region\", \"Date\", \"ConfirmedCases\", \"Fatalities\"]],\n",
    "                                                                         how=\"left\", on=[\"Province_State\",\"Country_Region\", \"Date\"])\n",
    "fixed_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.concat([sub_test_df, fixed_test_df]).sort_values([\"Country_Region\", \"Province_State\", \"Date\"],\n",
    "                                                                 ascending=[True, True, True])\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = predict_df.reset_index()\n",
    "for i in range(len(predict_df)):\n",
    "    if pd.isnull(predict_df.iloc[i][\"ConfirmedCases\"]):\n",
    "        predict_df.loc[i, \"ConfirmedCases\"] = predict_df.iloc[i - 1][\"ConfirmedCases\"] + predict_df.iloc[i][\"NewCases\"]\n",
    "    if pd.isnull(predict_df.iloc[i][\"Fatalities\"]):\n",
    "        predict_df.loc[i, \"Fatalities\"] = predict_df.iloc[i - 1][\"Fatalities\"] + predict_df.iloc[i][\"NewFatalities\"]\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert predict_df.shape[0] == test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"India\"\n",
    "\n",
    "target = \"ConfirmedCases\"\n",
    "region_train_df = train_df[(train_df[\"Country_Region\"]==country)]\n",
    "region_predict_df = predict_df[(predict_df[\"Country_Region\"]==country)]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax1 = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "fig = px.line(region_train_df, x=\"Date\", y=target, title='Prediction in India')\n",
    "fig = px.line(region_predict_df, x=\"Date\", y=target, title='Prediction in India')\n",
    "fig.show()\n",
    "\n",
    "ax1.plot(region_train_df[\"Date\"],\n",
    "         region_train_df[target],\n",
    "         color=\"green\")\n",
    "\n",
    "ax1.plot(region_predict_df[\"Date\"],\n",
    "         region_predict_df[target],\n",
    "         color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
