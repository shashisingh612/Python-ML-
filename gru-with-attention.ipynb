{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-37e6d9ad222f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor_palette\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#in V34 we are eliminating the aux input. Correcting the training data.\n",
    "\n",
    "#Version 31 used Quora competition embedding data. The submit button did not work saying exterior \n",
    "# data not allowed. Can do internal analysis using gap-development, which is same as test.\n",
    "# Need to change training data source from gap-dev to gap-training. \n",
    "\n",
    "#Version 32 is the first attempted implementation of IsLayer.\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import string\n",
    "import keras\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "from math import floor\n",
    "import spacy\n",
    "import zipfile\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Concatenate, Add, Flatten, CuDNNLSTM\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import defaultdict\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "9eb4372a4b591bf1b84973a66c45c4c359f66ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A DET DT det\n",
      "few ADJ JJ amod\n",
      "days NOUN NNS npadvmod\n",
      "later ADV RB advmod\n",
      ", PUNCT , punct\n",
      "Abigail PROPN NNP nsubj\n",
      "complained VERB VBD ROOT\n",
      "that ADP IN mark\n",
      "Elizabeth PROPN NNP nsubj\n",
      "was VERB VBD aux\n",
      "pinching VERB VBG ccomp\n",
      "her PRON PRP dobj\n",
      "and CCONJ CC cc\n",
      "tearing VERB VBG conj\n",
      "at ADP IN prep\n",
      "her ADJ PRP$ poss\n",
      "bowels NOUN NNS pobj\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'A few days later, Abigail complained that Elizabeth was pinching her and tearing at her bowels')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e1fb278795718ac3a298bf27ae34744b0da628af"
   },
   "outputs": [],
   "source": [
    "def word_locate(sentence, location): \n",
    "    count_words = 0\n",
    "    count_chars = 2 #2 is to count for the two spaces in the beginning\n",
    "    for word in sentence.split():\n",
    "        count_words += 1\n",
    "        if location == count_chars:\n",
    "            return word, count_words\n",
    "        count_chars += len(word)\n",
    "        count_chars += 1 #for space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "b73b30ec281e0587d5bd230903ab721ae954188d"
   },
   "outputs": [],
   "source": [
    "def curr_prev_sentence(sentence, loc):\n",
    "    current_sentence = \"\"\n",
    "    prev_sentence = \"\"\n",
    "    detect = 0\n",
    "    count = 0\n",
    "    for char in sentence:\n",
    "        count += 1\n",
    "        current_sentence += char\n",
    "        if char == \".\" and detect == 0:\n",
    "            prev_sentence = current_sentence \n",
    "            current_sentence = \"\"\n",
    "        if char == \".\" and detect == 1:\n",
    "            return current_sentence, prev_sentence\n",
    "        if count == loc:\n",
    "            detect = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "66edc761e807b587a78847e81eb448f4efed644b"
   },
   "outputs": [],
   "source": [
    "def find_subject(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            return token.text\n",
    "    return \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "d9078fe9d5b8755cd23ed857cbb633faf14b8727"
   },
   "outputs": [],
   "source": [
    "text1 = \"After a few years of almost no work -- although he was a guest star on Lou Grant and Charlie's Angels in the late 1970s, he once summed up the 1970s as 'I cried and did a lot of gardening' -- he was hired in 1979 for his best-known role, self-made millionaire Palmer Cortlandt on ABC's long-running soap opera All My Children. Initially hired for only one year, he remained on contract through 2009. For much of his first decade on the show, Palmer was a ruthless villain, totally possessive of his daughter, Nina and violently threatening his ex-wife Daisy with being attacked by dobermans when she came back from the dead.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "a5e707081c46804ce9e4dabfd180372ed7542e6f"
   },
   "outputs": [],
   "source": [
    "text2 = \"When onlookers expressed doubt, claiming that the Proctor family was well regarded in the community, the girl promptly came out of her trance and told them it was all for 'sport'. On March 29, 1692, Abigail Williams and Mercy Lewis again said they were being tormented by Elizabeth's spectre. A few days later, Abigail complained that Elizabeth was pinching her and tearing at her bowels, and said she saw Elizabeth's spectre as well as John's.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "3a295443dede4624b9d965209e79df08991c50b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A few days later, Abigail complained that Elizabeth was pinching her and tearing at her bowels, and said she saw Elizabeth's spectre as well as John's.\n"
     ]
    }
   ],
   "source": [
    "current, prev = curr_prev_sentence(text2, 360)\n",
    "print(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "dc6f870198b7aa091e7f73124a4a72f3dd8c836a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abigail\n"
     ]
    }
   ],
   "source": [
    "candidate = find_subject(current)\n",
    "print(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "00c8e1b143837f77771231aeb730d2125eea6f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "her   60\n"
     ]
    }
   ],
   "source": [
    "word, loc = word_locate(text2, 360) \n",
    "print(word, \" \", loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "51a6db6e45786dcfdfd9e09fe35d2bd9a7be3efd"
   },
   "outputs": [],
   "source": [
    "with open('../input/gap-other-training-data/gap-training.tsv') as tsvfile:\n",
    "    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    tr_label_A_list = []\n",
    "    tr_label_B_list = []\n",
    "    for row in reader:\n",
    "        text = row['Text']\n",
    "        text = text.lower()\n",
    "        new_textA = text\n",
    "        labelA = 0\n",
    "        labelB = 0\n",
    "        check = 0 #to allign the text in correct location after first insert\n",
    "        for char_idx in range(len(text)):\n",
    "            if char_idx == int(row['A-offset']):\n",
    "                new_textA = new_textA[:char_idx+check] + \"person_loc \" + new_textA[char_idx+check:]\n",
    "                check = 11\n",
    "            if char_idx == int(row['Pronoun-offset']):\n",
    "                new_textA = new_textA[:char_idx+check] + \"pronom_loc \" + new_textA[char_idx+check:]\n",
    "                check = 11\n",
    "        if row['A-coref'] == 'TRUE':\n",
    "            labelA = 1\n",
    "        train_X.append(new_textA)\n",
    "        train_y.append(labelA)\n",
    "        new_textB = text\n",
    "        label = 0\n",
    "        check = 0 #to allign the text in correct location after first insert\n",
    "        for char_idx in range(len(text)):\n",
    "            if char_idx == int(row['B-offset']):\n",
    "                new_textB = new_textB[:char_idx+check] + \"person_loc \" + new_textB[char_idx+check:]\n",
    "                check = 11\n",
    "            if char_idx == int(row['Pronoun-offset']):\n",
    "                new_textB = new_textB[:char_idx+check] + \"pronom_loc \" + new_textB[char_idx+check:]\n",
    "                check = 11\n",
    "        if row['B-coref'] == 'TRUE':\n",
    "            labelB = 1\n",
    "\n",
    "        train_X.append(new_textB)\n",
    "        train_y.append(labelB)\n",
    "        \n",
    "        tr_label_A_list.append(labelA)\n",
    "        tr_label_B_list.append(labelB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "5c63bc70f54fc2ffbb6adbd6bf9e7d51bcddb2b6"
   },
   "outputs": [],
   "source": [
    "with open('../input/gap-other-training-data/gap-training.tsv') as tsvfile:\n",
    "    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
    "    testA_X = []\n",
    "    testA_y = []\n",
    "    testB_X = []\n",
    "    testB_y = []\n",
    "    label_A_list = []\n",
    "    label_B_list = []\n",
    "    for row in reader:\n",
    "        text = row['Text']\n",
    "        text = text.lower()\n",
    "        new_textA = text\n",
    "        labelA = 0\n",
    "        labelB = 0\n",
    "        check = 0 #to allign the text in correct location after first insert\n",
    "        for char_idx in range(len(text)):\n",
    "            if char_idx == int(row['A-offset']):\n",
    "                new_textA = new_textA[:char_idx+check] + \"person_loc \" + new_textA[char_idx+check:]\n",
    "                check = 11\n",
    "            if char_idx == int(row['Pronoun-offset']):\n",
    "                new_textA = new_textA[:char_idx+check] + \"pronom_loc \" + new_textA[char_idx+check:]\n",
    "                check = 11\n",
    "        if row['A-coref'] == 'TRUE':\n",
    "            labelA = 1\n",
    "        testA_X.append(new_textA)\n",
    "        testA_y.append(labelA)\n",
    "        new_textB = text\n",
    "        label = 0\n",
    "        check = 0 #to allign the text in correct location after first insert\n",
    "        for char_idx in range(len(text)):\n",
    "            if char_idx == int(row['B-offset']):\n",
    "                new_textB = new_textB[:char_idx+check] + \"person_loc \" + new_textB[char_idx+check:]\n",
    "                check = 11\n",
    "            if char_idx == int(row['Pronoun-offset']):\n",
    "                new_textB = new_textB[:char_idx+check] + \"pronom_loc \" + new_textB[char_idx+check:]\n",
    "                check = 11\n",
    "        if row['B-coref'] == 'TRUE':\n",
    "            labelB = 1\n",
    "\n",
    "        testB_X.append(new_textB)\n",
    "        testB_y.append(labelB)\n",
    "        \n",
    "        label_A_list.append(labelA)\n",
    "        label_B_list.append(labelB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "2c678ed34eb5d77e7b0de04943724a7aa87d7a1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "cea25a36ac6a1788e9940f5e697d392b525569e9"
   },
   "outputs": [],
   "source": [
    "maxlen = 120\n",
    "embed_size = 300\n",
    "max_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "a06bf47e120dfaebc313916c128b4674a3fcc218"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer_list = list(train_X)\n",
    "tokenizer.fit_on_texts(tokenizer_list)\n",
    "\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "testA_X = tokenizer.texts_to_sequences(testA_X)\n",
    "testB_X = tokenizer.texts_to_sequences(testB_X)\n",
    "\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "testA_X = pad_sequences(testA_X, maxlen=maxlen)\n",
    "testB_X = pad_sequences(testB_X, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "707cf46fdcb1264f5070e994432804e27915942c"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "max_features = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "953adbafc496dcbe7d536dda22ce6cffb41d4bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-24 06:49:43--  http://nlp.stanford.edu/data/glove.840B.300d.zip\r\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... failed: Temporary failure in name resolution.\r\n",
      "wget: unable to resolve host address ‘nlp.stanford.edu’\r\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "e1ba0b32643c09348cd25d83c055af43f84713df"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.840B.300d.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-abc973a11f6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove.840B.300d.zip\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ls 'cased_glove_300'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.840B.300d.zip'"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(\"glove.840B.300d.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "!ls 'cased_glove_300'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "e53d126512adb70789b24d92994e206a8a51a746"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if word == \"pronom_loc\":\n",
    "            embedding_vector = np.ones(300)*0.85 # the number is arbitrary, just some vector\n",
    "        if word == \"person_loc\":\n",
    "            embedding_vector = np.ones(300)*0.25 # the number is arbitrary, just some vector\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "    return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "29a89ba55a026942a997f713eb79e727c2ab5c88"
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "6d571cbae35966419a19a3e2a7d0763c156f4103"
   },
   "outputs": [],
   "source": [
    "def dupli_row(x, num_copies, axis):\n",
    "    y = x\n",
    "    for i in range(num_copies-1):\n",
    "        y = np.concatenate((y,x), axis= axis)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "c6ee96231564a5a7f756fafd5a9bf9bd4b52dff5"
   },
   "outputs": [],
   "source": [
    "class IsLayer(Layer):\n",
    "    #Layer to be used after a dense one. It will multiply all the elements with each other.\n",
    "    #In a sense, it allows the neurons to have a say on  each others' outputs. This layer, hopefully,\n",
    "    #compares the relative importance of neurons. A dominant neuron can have more say via its weight.\n",
    "    #The idea follows from attention layer, but is more basic than that. As it is multiplicative, it is \n",
    "    #an alternative to the vanilla additive layer where outputs are added at the next layer.\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(IsLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='W', \n",
    "                                 shape=(input_shape[1], 1), \n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(IsLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x_W = K.dot(x, self.W)\n",
    "        x_new = x*x_W \n",
    "        return x_new\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "fa8ebc8290956f118c9863325bffac150d56bf3c"
   },
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "f06972468cf9492bb1241253808ce6527832f59a"
   },
   "outputs": [],
   "source": [
    "train_y = np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "45f9e21e970f8307e863b1206b31fe788ecea6cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning:\n",
      "\n",
      "arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = load_glove(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "bffd67d714d5cb8b75559b444f99a39fdb65b18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 120, 300)          6783900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 120, 512)          857088    \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 512)               263168    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 7,920,605\n",
      "Trainable params: 1,136,705\n",
      "Non-trainable params: 6,783,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inp1 = Input(shape=(maxlen,))\n",
    "\n",
    "model1_out = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp1)\n",
    "model1_out = Bidirectional(CuDNNGRU(256, return_sequences=True))(model1_out)\n",
    "model1_out = AttentionWithContext()(model1_out)\n",
    "model1_out = Dropout(0.1)(model1_out)\n",
    "\n",
    "merged_out = Dense(32, activation=\"relu\")(model1_out)\n",
    "merged_out = Dropout(0.1)(merged_out)\n",
    "merged_out = Dense(1, activation=\"sigmoid\")(merged_out)\n",
    "model = Model(inputs=inp1, outputs=merged_out)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "adee9dcc23851cf063ff33c3d828d1c8c691a3e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4000 samples, validate on 4000 samples\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 3s 842us/step - loss: 0.6901 - acc: 0.5445 - val_loss: 0.6867 - val_acc: 0.5567\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 1s 246us/step - loss: 0.6887 - acc: 0.5465 - val_loss: 0.6861 - val_acc: 0.5567\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 1s 245us/step - loss: 0.6734 - acc: 0.5843 - val_loss: 0.6471 - val_acc: 0.6455\n",
      "Epoch 4/20\n",
      "4000/4000 [==============================] - 1s 246us/step - loss: 0.6455 - acc: 0.6367 - val_loss: 0.6181 - val_acc: 0.6672\n",
      "Epoch 5/20\n",
      "4000/4000 [==============================] - 1s 247us/step - loss: 0.6231 - acc: 0.6665 - val_loss: 0.5996 - val_acc: 0.6883\n",
      "Epoch 6/20\n",
      "4000/4000 [==============================] - 1s 248us/step - loss: 0.6052 - acc: 0.6865 - val_loss: 0.5825 - val_acc: 0.7068\n",
      "Epoch 7/20\n",
      "4000/4000 [==============================] - 1s 247us/step - loss: 0.5940 - acc: 0.6937 - val_loss: 0.5690 - val_acc: 0.7150\n",
      "Epoch 8/20\n",
      "4000/4000 [==============================] - 1s 247us/step - loss: 0.5630 - acc: 0.7195 - val_loss: 0.5356 - val_acc: 0.7457\n",
      "Epoch 9/20\n",
      "4000/4000 [==============================] - 1s 248us/step - loss: 0.5433 - acc: 0.7425 - val_loss: 0.5410 - val_acc: 0.7305\n",
      "Epoch 10/20\n",
      "4000/4000 [==============================] - 1s 248us/step - loss: 0.5405 - acc: 0.7382 - val_loss: 0.5121 - val_acc: 0.7582\n",
      "Epoch 11/20\n",
      "4000/4000 [==============================] - 1s 245us/step - loss: 0.5190 - acc: 0.7505 - val_loss: 0.5043 - val_acc: 0.7575\n",
      "Epoch 12/20\n",
      "4000/4000 [==============================] - 1s 247us/step - loss: 0.5046 - acc: 0.7665 - val_loss: 0.5010 - val_acc: 0.7442\n",
      "Epoch 13/20\n",
      "4000/4000 [==============================] - 1s 248us/step - loss: 0.4848 - acc: 0.7720 - val_loss: 0.4549 - val_acc: 0.8008\n",
      "Epoch 14/20\n",
      "4000/4000 [==============================] - 1s 247us/step - loss: 0.4503 - acc: 0.8008 - val_loss: 0.4127 - val_acc: 0.8167\n",
      "Epoch 15/20\n",
      "4000/4000 [==============================] - 1s 248us/step - loss: 0.4240 - acc: 0.8132 - val_loss: 0.4007 - val_acc: 0.8193\n",
      "Epoch 16/20\n",
      "4000/4000 [==============================] - 1s 248us/step - loss: 0.4166 - acc: 0.8105 - val_loss: 0.3739 - val_acc: 0.8333\n",
      "Epoch 17/20\n",
      "4000/4000 [==============================] - 1s 248us/step - loss: 0.4021 - acc: 0.8175 - val_loss: 0.3548 - val_acc: 0.8400\n",
      "Epoch 18/20\n",
      "4000/4000 [==============================] - 1s 248us/step - loss: 0.3568 - acc: 0.8423 - val_loss: 0.3332 - val_acc: 0.8570\n",
      "Epoch 19/20\n",
      "4000/4000 [==============================] - 1s 247us/step - loss: 0.3387 - acc: 0.8570 - val_loss: 0.3076 - val_acc: 0.8735\n",
      "Epoch 20/20\n",
      "4000/4000 [==============================] - 1s 246us/step - loss: 0.3222 - acc: 0.8585 - val_loss: 0.3053 - val_acc: 0.8645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78163d3978>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, batch_size=512, epochs=20, validation_data=(train_X, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "46dfe12b5c7bb75ba8ce76e596b09c66e8beed97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 136us/step\n",
      "2000/2000 [==============================] - 0s 67us/step\n"
     ]
    }
   ],
   "source": [
    "pred_valA_y = model.predict(testA_X, batch_size=512, verbose=1)\n",
    "pred_valB_y = model.predict(testB_X, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "fcece3985792df7fbbb083965dda054432ee71bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000   2000\n"
     ]
    }
   ],
   "source": [
    "print(len(pred_valB_y), \" \", len(pred_valA_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "d9ddbb1b1af6369b5ed12383977c1ce147d1307f"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([[0.01749572],\n       [0.96493477],\n       [0.15171367],\n       ...,\n       [0.94990075],\n       [0.8518851 ],\n       [0.25072706]], dtype=float32),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ce7deacee47c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test score A:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_valA_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestA_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1769\u001b[0m         \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1771\u001b[0;31m         \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (array([[0.01749572],\n       [0.96493477],\n       [0.15171367],\n       ...,\n       [0.94990075],\n       [0.8518851 ],\n       [0.25072706]], dtype=float32),)"
     ]
    }
   ],
   "source": [
    "print(\"Test score A:\", log_loss(pred_valA_y,testA_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "64853cd10da9443dcb5fbf4bf4bda840f29b7021"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([[0.26928896],\n       [0.02994871],\n       [0.14222583],\n       ...,\n       [0.01984984],\n       [0.00883377],\n       [0.06046918]], dtype=float32),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8e9e0c254a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test score B:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_valB_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestB_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1769\u001b[0m         \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1771\u001b[0;31m         \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (array([[0.26928896],\n       [0.02994871],\n       [0.14222583],\n       ...,\n       [0.01984984],\n       [0.00883377],\n       [0.06046918]], dtype=float32),)"
     ]
    }
   ],
   "source": [
    "print(\"Test score B:\", log_loss(pred_valB_y,testB_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "61eb913d49b4bd366ff3449e9f34e2f125beb6ff"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-99af99a5331e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ID\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_ids' is not defined"
     ]
    }
   ],
   "source": [
    "out_df = pd.DataFrame({\"ID\":test_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "4994d9544f504fbbb00fef6ed318968023c6c4b8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c46fbccfe21e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_valA_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mout_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_valB_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out_df' is not defined"
     ]
    }
   ],
   "source": [
    "out_df['A'] = [max(float(val),0.33) for val in list(pred_valA_y)]\n",
    "out_df['B'] = [max(float(val),0.33) for val in list(pred_valB_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "b8f90b9a3d278851927f47404b8353689b0562b1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a94fd2c149f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NEITHER'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_valA_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'out_df' is not defined"
     ]
    }
   ],
   "source": [
    "out_df['NEITHER'] = [max((1-float(val)),0.33) for val in list(pred_valA_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "157c02ca36df5746454359dbe120e410be45519e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7e25016e8ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'out_df' is not defined"
     ]
    }
   ],
   "source": [
    "out_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
