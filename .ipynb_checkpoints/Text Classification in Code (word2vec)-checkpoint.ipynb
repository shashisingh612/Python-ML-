{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:\\\\Users\\\\ac36345\\\\Desktop\\\\nlp code\\\\r8-train-all-terms.txt',header=None,sep='\\t')\n",
    "test = pd.read_csv('C:\\\\Users\\\\ac36345\\\\Desktop\\\\nlp code\\\\r8-test-all-terms.txt',header=None,sep='\\t')\n",
    "train.columns = ['label','contents']\n",
    "test.columns = ['label','contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer:\n",
    "    def __init__(self):\n",
    "        print('loadind word vectors...')\n",
    "        word2vec = {}\n",
    "        embedding = []\n",
    "        idx2word = []\n",
    "        with open('C:\\\\Users\\\\ac36345\\\\Desktop\\\\nlp code\\\\glove.6B\\\\glove.6B.50d.txt',encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vec = np.asarray(values[1:],dtype='float32')\n",
    "                word2vec[word] = vec\n",
    "                embedding.append(vec)\n",
    "                idx2word.append(word)\n",
    "        print('found %s word vectors.' %len(word2vec))\n",
    "        self.word2vec= word2vec\n",
    "        self.embedding = np.array(embedding)\n",
    "        self.word2idx = {v:k for k,v in enumerate(idx2word)}\n",
    "        self.V, self.D = self.embedding.shape\n",
    "    \n",
    "    def fit(self,data):\n",
    "        pass\n",
    "                        \n",
    "    def transform(self,data):\n",
    "        X = np.zeros((len(data),self.D))\n",
    "        n = 0\n",
    "        emptycount = 0\n",
    "        for sentence in data:\n",
    "            tokens = sentence.lower().split()\n",
    "            vecs = []\n",
    "            for word in tokens:\n",
    "                if word in self.word2vec:\n",
    "                    vec = self.word2vec\n",
    "                    vecs.append(vec)\n",
    "            if len(vecs) > 0:\n",
    "                vecs = np.array(vecs)\n",
    "                X[n] = vecs.mean(axis=0)\n",
    "            else:\n",
    "                emptycount +=1\n",
    "            n += 1\n",
    "        print('number of samples with no words found: %s / %s ' %(emptycount,len(data)))\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self,data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "    \n",
    "                        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec\n",
    "class word2VecVectorizer:\n",
    "    def __init__(self):\n",
    "        print('loadind word vectors...')\n",
    "        self.word_vectors = KeyedVectors.load_word2vec_format('C:\\\\Users\\\\ac36345\\\\Desktop\\\\GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
    "        print(\"finished loading in word vectors\")\n",
    "    \n",
    "    def fit(self,data):\n",
    "        pass\n",
    "                        \n",
    "    def transform(self,data):\n",
    "        v = self.word_vectors.get_vector('king')\n",
    "        self.D = v.shape[0]\n",
    "        X = np.zeros((len(data),self.D))\n",
    "        n = 0\n",
    "        emptycount = 0\n",
    "        for sentence in data:\n",
    "            tokens = sentence.lower().split()\n",
    "            vecs = []\n",
    "            m=0\n",
    "            for word in tokens:\n",
    "                try:\n",
    "                    vec = self.word_vectors.get_vector(word)\n",
    "                    vecs.append(vec)\n",
    "                    m +=1\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if len(vecs) > 0:\n",
    "                vecs = np.array(vecs)\n",
    "                X[n] = vecs.mean(axis=0)\n",
    "            else:\n",
    "                emptycount +=1\n",
    "            n += 1\n",
    "        print('number of samples with no words found: %s / %s ' %(emptycount,len(data)))\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self,data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadind word vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ac36345\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading in word vectors\n",
      "number of samples with no words found: 0 / 5485 \n",
      "number of samples with no words found: 0 / 1152 \n"
     ]
    }
   ],
   "source": [
    "vectorizer = word2VecVectorizer()\n",
    "Xtrain = vectorizer.fit_transform(train.contents)\n",
    "Ytrain = train.label\n",
    "\n",
    "Xtest = vectorizer.transform(test.contents)\n",
    "Ytest = test.label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9992707383773929\n",
      "test score: 0.9375\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=200)\n",
    "model.fit(Xtrain,Ytrain)\n",
    "print('train score:',model.score(Xtrain,Ytrain))\n",
    "print('test score:',model.score(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
