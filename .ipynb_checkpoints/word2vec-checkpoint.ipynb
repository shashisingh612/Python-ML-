{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished counting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ac36345\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:144: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\ac36345\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:145: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fa6d856ad4ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m     \u001b[0mword2idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'w2v_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m     \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-fa6d856ad4ae>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(savedir)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mp_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_negative_sampling_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mtotal_words\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-fa6d856ad4ae>\u001b[0m in \u001b[0;36mget_negative_sampling_distribution\u001b[1;34m(sentences, vocab_size)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mp_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_neg\u001b[0m \u001b[1;33m/\u001b[0m\u001b[0mp_neg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_neg\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mp_neg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function,division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "from gensim.models import KeyedVectors\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit as sigmoid\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from scipy.spatial.distance import cosine as cos_dist\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "#from rnn_class.brown import get_sentences_with_word2idx_limit_vocb as get-brown\n",
    "\n",
    "def remove_punctuation_2(s):\n",
    "    return s.translate(None,string.punctuation)\n",
    "\n",
    "def remove_punctuation_3(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "\n",
    "\n",
    "if sys.version.startswith('2'):\n",
    "    remove_punctuation = remove_punctuation_2\n",
    "else:\n",
    "    remove_punctuation = remove_punctuation_3\n",
    "\n",
    "def get_wiki():\n",
    "    V = 20000\n",
    "    files = glob('../large_files/enwiki*.txt')\n",
    "    all_word_counts = {}\n",
    "    for f in files:\n",
    "       for line in open(f):\n",
    "           if line and line[0] not in '[*-|=\\{\\}':\n",
    "               s = remove_punctuation(line).lower().split()\n",
    "               if len(s) > 1:\n",
    "                  for word in s:\n",
    "                       if word not in all-word_counts:\n",
    "                           all_word_counts[word] = 0\n",
    "                       all_word_counts[word] +=1\n",
    "    print(\"finished counting\")\n",
    "    V = min(V,len(all_word_counts))\n",
    "    all_word_counts = sorted(all_word_counts.items(),key =lambda x: x[1],reverse=True)\n",
    "    top_words = [w for w,count in all_word_counts[:V-1]] +['<UNK>']\n",
    "    word2idx = {w:i for i,w in enumerate(top_words)}\n",
    "    unk = word2idx['<UNK>']\n",
    "    sents = []\n",
    "    for f in files:\n",
    "        for line in open(f):\n",
    "            if line and line[0] not in '[*-|=\\{\\}':\n",
    "                s = remove_punctuation(line).lower().split()\n",
    "                if len(s) > 1:\n",
    "                    sent =[word2idx[w] if w in word2idx else unk for w in s]\n",
    "                    sents.append(sent)\n",
    "    return sents,word2idx\n",
    "\n",
    "\n",
    "def train_model(savedir):\n",
    "    sentences,word2idx = get_wiki()\n",
    "    vocab_size = len(word2idx)\n",
    "    window_size = 5\n",
    "    learning_rate = 0.025\n",
    "    final_learning_rate = 0.0001\n",
    "    num_negatives = 5\n",
    "    epochs = 20\n",
    "    D = 50\n",
    "    learning_rate_delte = (learning_rate - final_learning_rate) / epochs\n",
    "\n",
    "    W = np.random.randn(vocab_size,D)\n",
    "    V = np.random.randn(D, vocab_size)\n",
    "    \n",
    "    p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n",
    "    costs = []\n",
    "    total_words =  sum(len(sentence) for sentence in sentences)\n",
    "    print(\"total number of words in corpus:\",total_words)\n",
    "    threshold = 1e-5\n",
    "    p_drop = 1- np.sqrt(threshold / p_neg)\n",
    "   \n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(sentences)\n",
    "        cost = 0\n",
    "        counter = 0\n",
    "        t0 =  datetime.now()\n",
    "        for sentence in sentences:\n",
    "            sentence = [W for W in sentence \\\n",
    "                        if np.random.random() < (1-p_drop[w])\n",
    "            ]\n",
    "            if len(sentence) < 2:\n",
    "                continue\n",
    "            randomly_ordered_positions = np.random.choice(\n",
    "               len(sentence),\n",
    "               size = len(sentence),\n",
    "               replace = False,\n",
    "            )\n",
    "            \n",
    "            for pos in rndomly_ordered_positions:\n",
    "                word = sentence[pos]\n",
    "                context_words = get_context(pos, sentence, window_size)\n",
    "                neg_word = np.random.choice(vocab_size, p=p_neg)\n",
    "                targets = np.array(context_words)\n",
    "                c = sgd(word, targets,1, learning_rate,W,V)\n",
    "                cost +=c\n",
    "                c = sgd(neg_word, targets,0,learning_rtae,W,V)\n",
    "                cost +=c\n",
    "\n",
    "            counter +=1\n",
    "            if counter % 100 ==0:\n",
    "                sys.stdout.write(\"processed %s / %s\\r\" %(counter, len(sentences)))\n",
    "                sys.stdout.flush()\n",
    "        dt = datetime.now() - t0\n",
    "        print(\"epoch complete:\", epoch,\"cost:\", cost,\"dt:\",dt)\n",
    "\n",
    "        costs.append(cost)\n",
    "        learning_rate -= learning_rate_delta\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    if not os.pth.exists(savedir):\n",
    "        os.mkdir(savedir)\n",
    "\n",
    "    with open('%s/word2idx.json'%savedir,'w') as f:\n",
    "      json.dump(word2idx,f)\n",
    "\n",
    "    np.savez('%s/weights.npz' %savedir,W,V)\n",
    "    return word2idx,W,V\n",
    "\n",
    "\n",
    "def get_negative_sampling_distribution(sentences,vocab_size):\n",
    "    word_freq = np.zeros(vocab_size)\n",
    "    word_count =  sum(len(sentence) for sentence in sentences)\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            word_freq[word] +=1\n",
    "\n",
    "    p_neg = word_freq ** 0.75\n",
    "\n",
    "    p_neg = p_neg /p_neg.sum()\n",
    "    assert(np.all(p_neg>0))\n",
    "    return p_neg\n",
    "\n",
    "\n",
    "def get_contect(pos, sentence, window_size):\n",
    "    start = max(0,pos - window_size)\n",
    "    end_ = min(len(sentence),pos+window_size)\n",
    "\n",
    "    context = []\n",
    "    for ctx_pos, ctx_word_idx in enumerate(sentence[start:end_],start= start):\n",
    "        if ctx_pos != pos:\n",
    "           context.append(ctx_word_idx)\n",
    "    return context\n",
    "\n",
    "def sgd(input_,targets,label,learning_rate,W,V):\n",
    "    activation =  W[input_].dot(V[:,targets])\n",
    "    prob = sigmoid(activation)\n",
    "\n",
    "    gV = np.outer(W[input_],prob-label)\n",
    "    gW = np.sum((prob - label)*V[:,targets],axis=1)\n",
    "\n",
    "    V[:,targtes] -= learning_rate*gV\n",
    "    W[input_] -=learning_rate*gW\n",
    "\n",
    "    cost = label * np.log(prob + 1e-10) + (1-label) * np.log(1- prob +1e-10)\n",
    "    return cost.sum()\n",
    "\n",
    "def load_model(savedir):\n",
    "    with open('%s/word2idx.json' % savedir) as f:\n",
    "      word2idx = json.load(f)\n",
    "    npz = np.load('%s/weights.npz' % savedir)\n",
    "    W = npz['arr_0']\n",
    "    V = npz['arr_1']\n",
    "    return word2idx,W,V\n",
    "\n",
    "\n",
    "def analogy(pos1,neg1,pos2neg2,word2idx,ix2word,w):\n",
    "    V,D = W.shape\n",
    "    print(\"testing: %s - %s = %s - %s\" %(pos1,neg1,pos2,neg2))\n",
    "    for w in (pos1,neg1,pos2,neg2):\n",
    "        if w not in word2idx:\n",
    "            print(\"sorry, %s not in word2idx\" %w)\n",
    "    p1 = W[word2idx[pos1]] \n",
    "    n1 = W[word2idx[neg1]] \n",
    "    p2 = W[word2idx[pos2]] \n",
    "    n2 = W[word2idx[neg2]] \n",
    "\n",
    "    vec = p1 - n1 + n2\n",
    "    distances = pairwise_distances(vec.reshape(1,D),W,meric='cosine').reshape(V)\n",
    "    idx = distance.argsort()[:10]\n",
    "\n",
    "    best_idx = -1\n",
    "    keep_out = [word2idx[w] for w in (pos1,neg1,neg2)]\n",
    "\n",
    "    for i in idx:\n",
    "        if i not in keep_out:\n",
    "           best_idx = i\n",
    "           break\n",
    "    print(\"got: %s - %s =  %s - %s\" % (pos1,neg1,idx2word[best_idx],neg2))\n",
    "    print(\"closet 10:\")\n",
    "    for i in idx:\n",
    "        print(idx2wrd[i],distances[i])\n",
    "    print(\"dist to %s:\" %pos2,cos_dist(p2,vec))\n",
    "\n",
    "def test_model(word2idx,W,V):\n",
    "    idx2word = {i:w for w,i in word2idx.items()}\n",
    "    for We in (W, (W+V.T)/2):\n",
    "        print(\"*************\")\n",
    "        \n",
    "        analogy('king','man','queen','woman',word2idx,idx2word,We)\n",
    "        analogy('king','prince','queen','princess',word2idx,idx2word,We)\n",
    "        analogy('miami','florida','della','texas',word2idx,idx2word,We)\n",
    "        analogy('einstein','scientist','picasso','painter',word2idx,idx2word,We)\n",
    "        analogy('japan','sushi','germany','bratwurst',word2idx,idx2word,We)\n",
    "        analogy('man','woman','he','she',word2idx,idx2word,We)\n",
    "        analogy('man','woman','uncle','aunt',word2idx,idx2word,We)\n",
    "        analogy('man','woman','broher','sister',word2idx,idx2word,We)\n",
    "        analogy('man','woman','husband','wife',word2idx,idx2word,We)\n",
    "        analogy('man','woman','actor','actress',word2idx,idx2word,We)\n",
    "        analogy('man','woman','father','mother',word2idx,idx2word,We)\n",
    "        analogy('heir','heiress','prince','princess',word2idx,idx2word,We)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    word2idx,W,V = train_model('w2v_model')\n",
    "    test_model(word2idx,W,V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "from builtins import range\n",
    "from nltk.corpus import brown\n",
    "import operator\n",
    "\n",
    "KEEP_WORDS = set([\n",
    "'king','man','queen','woman',\n",
    "'italy','rome','france','paris',\n",
    "'london','britain','england',])\n",
    "\n",
    "def get_sentences():\n",
    "    return brown.sents()\n",
    "\n",
    "def get_sentences_with_word2idx():\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "    i = 2\n",
    "    word2idx = {'START':0,'END': 1}\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = i\n",
    "                i +=1\n",
    "            indexed_sentence.append(word2idx[token])\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "    print('vocab size:',i)\n",
    "    return indexed_sentences,word2idx\n",
    "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000,keep_words=KEEP_WORDS):\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "    i = 2\n",
    "    word2idx = {'START':0,'END': 1}\n",
    "    idx2word = ['START','END']\n",
    "    word_idx_count = {\n",
    "      0: float('inf'),\n",
    "      1: float('inf'),\n",
    "    }\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                idx2word.append(token)\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "            idx = word2idx[token]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx,0) + 1\n",
    "            indexed_sentence.append(idx)\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "    for word in keep_words:\n",
    "        word_idx_count[word2idx[word]] = float('inf')\n",
    "        # sort technique operator.itemgetter(1)\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key = operator.itemgetter(1),reverse = True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "\n",
    "    for idx,count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        print(word, count)\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx +=1\n",
    "    word2idx_small['UNKNOWN'] = new_idx\n",
    "    unknown = new_idx\n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    for word in keep_words:\n",
    "        assert(word in word2idx_small)\n",
    "    sentences_small = []\n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "    return sentences_small, word2idx_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
