{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14fe983-de65-4e22-a250-f340e008751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(directory_path, character_name):\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        if os.path.isfile(file_path):  # Ignore directories\n",
    "            extract_character_lines(file_path, character_name)\n",
    "            \n",
    "    with open(f'./{character_name}_lines.jsonl', 'w', newline='') as outfile:\n",
    "        prevLine = ''\n",
    "        for s in character_lines:\n",
    "            if (s.startswith('DATA:')):\n",
    "                outfile.write(\"{\\\"messages\\\": [{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Data is an android in the TV series Star Trek: The Next Generation.\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"\" + prevLine + \"\\\"}, {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"\" + s + \"\\\"}]}\\n\")\n",
    "            prevLine = s\n",
    "\n",
    "def extract_character_lines(file_path, character_name):\n",
    "    with open(file_path, 'r') as script_file:\n",
    "        lines = script_file.readlines()\n",
    "\n",
    "    is_character_line = False\n",
    "    current_line = ''\n",
    "    current_character = ''\n",
    "    for line in lines:\n",
    "        strippedLine = line.strip()\n",
    "        if (is_single_word_all_caps(strippedLine)):\n",
    "            is_character_line = True\n",
    "            current_character = strippedLine\n",
    "        elif (line.strip() == '') and is_character_line:\n",
    "            is_character_line = False\n",
    "            dialog_line = strip_parentheses(current_line).strip()\n",
    "            dialog_line = dialog_line.replace('\"', \"'\")\n",
    "            character_lines.append(current_character + \": \" + dialog_line)\n",
    "            current_line = ''\n",
    "        elif is_character_line:\n",
    "            current_line += line.strip() + ' '\n",
    "            \n",
    "def split_file(input_filename, train_filename, eval_filename, split_ratio=0.8, max_lines=10000):\n",
    "    \"\"\"\n",
    "    Splits the lines of the input file into training and evaluation files.\n",
    "\n",
    "    :param input_filename: Name of the input file to be split.\n",
    "    :param train_filename: Name of the output training file.\n",
    "    :param eval_filename: Name of the output evaluation file.\n",
    "    :param split_ratio: Ratio of lines to be allocated to training. Default is 0.8, i.e., 80%.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(input_filename, 'r') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    # Shuffle lines to ensure randomness\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    lines = lines[:max_lines]\n",
    "\n",
    "    # Calculate the number of lines for training\n",
    "    train_len = int(split_ratio * len(lines))\n",
    "\n",
    "    # Split the lines\n",
    "    train_lines = lines[:train_len]\n",
    "    eval_lines = lines[train_len:]\n",
    "\n",
    "    # Write to the respective files\n",
    "    with open(train_filename, 'w') as trainfile:\n",
    "        trainfile.writelines(train_lines)\n",
    "\n",
    "    with open(eval_filename, 'w') as evalfile:\n",
    "        evalfile.writelines(eval_lines)\n",
    "\n",
    "process_directory('e:/Downloads23/scripts_tng', 'DATA')\n",
    "split_file('./DATA_lines.jsonl', './DATA_train.jsonl', './DATA_eval.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
